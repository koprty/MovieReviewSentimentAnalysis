{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Analysis Outline\n",
    "## import necessary python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pyspark' from '/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# \"transformData.py\" is this script which is intended to be run once on our data\n",
    "# transform the raw data into a rdd readable format first\n",
    "\n",
    "# import all necessary libraries\n",
    "import re\n",
    "import string\n",
    "from operator import add\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pyspark.mllib.regression as mllib_reg\n",
    "import pyspark.mllib.linalg as mllib_lalg\n",
    "import pyspark.mllib.classification as mllib_class\n",
    "import pyspark.mllib.tree as mllib_tree\n",
    "\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS , LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (pyspark) # test to see that pyspark is up and running okay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of all html tags in the data\n",
    "def strip_html_tags(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data.lower())\n",
    "\n",
    "#parse and take care of funky symbols in summary and text <- normalize to root words maybe?\n",
    "def cleanData (data):\n",
    "    data = strip_html_tags(data)\n",
    "    data =  re.sub(\"[\\t\\,\\:;\\(\\)\\\"\\'\\~\\-\\!\\?\\`]\", \"\",data, 0, 0)\n",
    "    #data =  re.sub(\"[\\.0`]\", \"\",data, 0, 0) # special case to get rid of \".0\" of scores -- \n",
    "    return data\n",
    "\n",
    "#re.sub(\"[\\.\\t\\,\\:;\\(\\)\\.]\", \" \", strip_html_tags(data.lower()), 0, 0)\n",
    "\n",
    "# clean the data for each element of each sub list\n",
    "def prepData (list_str):\n",
    "    L= []\n",
    "    for x in list_str:\n",
    "        #print x\n",
    "        L.append(cleanData(x).strip())\n",
    "    return L\n",
    "#####\n",
    "# Notes \n",
    "# headers for the project\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE THE FILE WE WANT TO USE AS OUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# transform Data uses /// as a separator for each eleent of data for each movie review\n",
    "# the original file for the current output.txt is a truncated version of all our data, so the end elements will be funky\n",
    "\n",
    "# output < smaller_parsed\n",
    "\n",
    "movies_txt2 = sc.textFile(\"100k_parsed.txt\").map(lambda x: (cleanData(x).split('///')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parsing out relevant fields - score and text of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99991, 100000)\n",
      "('Number of datapoints in dataset to train: ', 99991)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# prep our data\n",
    "\n",
    "#####\n",
    "# Notes \n",
    "# headers for the project\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "# [u'productId', _ , _ , _ , u'score' [0] to get rid of .0, _  u'summary', u'text']\n",
    "\n",
    "# take the data that we care most about\n",
    "movies_new = movies_txt2.map(lambda L: (L[0], L[4][0], L[6], L[7]) if len(L) == 8 else L )\n",
    "movies_new1 = movies_new.filter(lambda L: len(L) == 4) # lets just take all the data that has been parsed correctly\n",
    "\n",
    "#print (movies_new.top(20))\n",
    "print (movies_new1.count(), movies_new.count())\n",
    "print (\"Number of datapoints in dataset to train: \", movies_new1.count())\n",
    "print (movies_new.count() == movies_new1.count())\n",
    "\n",
    "removeHTMLTags = movies_new1.map(prepData) # remove html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVISED STOP WORDS REMOVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, [u'zulu', u'superlative', u'depiction', u'19th', u'century', u'battle', u'rourkes', u'drift', u'british', u'outpost', u'heavily', u'outnumbered', u'seige', u'zulu', u'warriors', u'fought', u'desperately', u'almost', u'last', u'man', u'somehow', u'prevailed', u'terrifying', u'exciting', u'war', u'film', u'unlike', u'splendid', u'cast', u'word', u'diamond', u'entertainment', u'tapesmy', u'experience', u'slp', u'speed', u'vhs', u'videos', u'tracks', u'remarkably', u'well', u'usually', u'recorded', u'hifi', u'sound', u'well', u'standard', u'linear', u'format', u'zulu', u'also', u'available', u'dvd', u'also', u'recommendedzulu', u'dawn', u'1979', u'tells', u'complete', u'story', u'struggles', u'tribesmen', u'british', u'soldiers', u'commenced', u'1879', u'burt', u'lancaster', u'vhs', u'edition', u'dvd', u'editionparenthetical', u'number', u'preceding', u'title', u'1', u'10', u'viewer', u'poll', u'rating', u'found', u'film', u'resource', u'website', u'8', u'0', u'zulu', u'uk1964', u'stanley', u'baker', u'jack', u'hawkins', u'ulla', u'jacobson', u'james', u'booth', u'michael', u'caine', u'nigel', u'green', u'patrick', u'magee', u'richard', u'burton', u'narrator'])]\n"
     ]
    }
   ],
   "source": [
    "#REVISED STOP WORDS REMOVAL Approach\n",
    "\n",
    "#used the stopwords file in the virtual box instead... taking too long\n",
    "baseDir = os.path.join('../data')\n",
    "inputPath = os.path.join('cs100', 'lab3')\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    #reusing assignment code\n",
    "    \"\"\" An implementation of input string tokenization to exclude stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    splitList = re.split(split_regex, string.lower().strip())\n",
    "    \n",
    "    return [x for x in splitList if x!=\"\" and x not in stopwords]  \n",
    "\n",
    "\n",
    "\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int(x[1]), tokenize(x[3]))  )\n",
    "print removeSW_output.top(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing - making the vector of features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, [u'zulu', u'superlative', u'depiction', u'19th', u'century', u'battle', u'rourkes', u'drift', u'british', u'outpost', u'heavily', u'outnumbered', u'seige', u'zulu', u'warriors', u'fought', u'desperately', u'almost', u'last', u'man', u'somehow', u'prevailed', u'terrifying', u'exciting', u'war', u'film', u'unlike', u'splendid', u'cast', u'word', u'diamond', u'entertainment', u'tapesmy', u'experience', u'slp', u'speed', u'vhs', u'videos', u'tracks', u'remarkably', u'well', u'usually', u'recorded', u'hifi', u'sound', u'well', u'standard', u'linear', u'format', u'zulu', u'also', u'available', u'dvd', u'also', u'recommendedzulu', u'dawn', u'1979', u'tells', u'complete', u'story', u'struggles', u'tribesmen', u'british', u'soldiers', u'commenced', u'1879', u'burt', u'lancaster', u'vhs', u'edition', u'dvd', u'editionparenthetical', u'number', u'preceding', u'title', u'1', u'10', u'viewer', u'poll', u'rating', u'found', u'film', u'resource', u'website', u'8', u'0', u'zulu', u'uk1964', u'stanley', u'baker', u'jack', u'hawkins', u'ulla', u'jacobson', u'james', u'booth', u'michael', u'caine', u'nigel', u'green', u'patrick', u'magee', u'richard', u'burton', u'narrator'])]\n"
     ]
    }
   ],
   "source": [
    "# tokenize our training set and testing set\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int(x[1]), tokenize(x[3]))  )\n",
    "print removeSW_output.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, SparseVector(200, {2: 2.3367, 5: 1.432, 7: 1.2508, 8: 1.4712, 9: 4.5551, 14: 1.3862, 15: 4.3128, 16: 3.5716, 17: 2.5615, 20: 4.4453, 21: 1.2526, 22: 1.1598, 25: 2.8899, 28: 4.1005, 35: 1.5192, 36: 1.746, 37: 1.8737, 41: 2.7183, 42: 1.3485, 45: 2.0726, 46: 1.8449, 47: 1.9309, 49: 1.5994, 51: 1.4061, 54: 1.5251, 59: 1.4766, 60: 1.06, 63: 1.3453, 64: 2.4572, 66: 2.5467, 67: 1.8286, 69: 1.309, 72: 5.3366, 75: 1.3871, 76: 1.3512, 77: 4.8183, 79: 1.0238, 80: 4.3744, 83: 4.8297, 87: 3.4432, 88: 1.3486, 92: 1.5477, 96: 1.8405, 97: 1.712, 100: 1.4397, 101: 3.231, 102: 1.55, 103: 5.1782, 104: 1.0877, 107: 1.1457, 109: 1.5145, 110: 2.233, 111: 4.678, 112: 4.1476, 113: 1.3248, 115: 1.4954, 116: 3.1415, 117: 2.3327, 119: 1.4568, 120: 1.3024, 121: 3.3429, 122: 1.8031, 123: 1.3322, 124: 1.1648, 125: 4.3824, 127: 4.7447, 131: 1.1393, 132: 1.7233, 133: 0.8136, 137: 2.6335, 139: 1.93, 143: 3.5102, 145: 2.3686, 149: 3.768, 151: 3.892, 154: 1.5942, 157: 1.5548, 158: 3.0743, 161: 1.4814, 162: 1.3929, 165: 1.8219, 166: 1.4293, 170: 1.1783, 173: 2.3779, 180: 3.1325, 181: 1.8715, 182: 1.8422, 183: 1.206, 184: 1.8014, 185: 1.0151, 186: 3.9447, 188: 2.1489, 190: 1.757, 191: 2.4332, 194: 1.4713, 197: 1.0227, 199: 2.2282})), (1.0, SparseVector(200, {2: 1.5578, 3: 2.0115, 7: 1.2508, 8: 1.4712, 10: 3.9798, 12: 0.6387, 33: 2.3173, 38: 1.5283, 43: 0.9986, 44: 1.3003, 45: 1.0363, 47: 0.9655, 50: 2.3243, 53: 2.3159, 61: 2.2173, 66: 1.6978, 68: 0.7608, 70: 2.785, 76: 1.3512, 78: 0.7589, 79: 1.0238, 84: 1.1752, 98: 1.3064, 100: 1.4397, 103: 2.5891, 106: 2.517, 112: 1.3825, 113: 1.656, 120: 3.9072, 123: 5.3287, 129: 1.2287, 135: 3.6151, 140: 4.8774, 141: 5.6926, 147: 1.562, 150: 1.7092, 153: 1.6264, 158: 1.5371, 161: 1.4814, 162: 1.3929, 163: 1.1579, 164: 1.615, 170: 1.1783, 172: 1.7086, 177: 1.3895, 180: 1.5663, 184: 3.6027, 192: 1.1033, 197: 1.0227}))]\n",
      "\n",
      "[LabeledPoint(1.0, (200,[0,1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,53,55,56,57,58,59,60,61,63,64,65,66,67,68,69,70,71,72,73,74,76,77,78,79,80,81,82,83,84,86,87,88,89,90,91,92,93,94,96,97,98,101,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,121,123,124,125,126,127,128,129,130,131,133,134,135,136,137,138,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,161,162,163,164,165,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],[4.12202743644,5.77164348228,3.89454165119,5.02876288668,3.38746936504,1.43204889426,7.76654221691,8.75591259215,7.59187630761,1.32658788114,1.30235126491,5.10923802017,3.11809019072,4.15852307616,1.0782003865,5.35739040861,2.56147569798,2.80655827309,7.41560906344,5.92702625856,3.75788390042,1.15984401829,3.15552469529,2.88989154597,9.60393187708,1.55890246575,5.46737278496,1.23807025293,4.45624830012,1.67635277806,2.7757915414,8.11065169966,3.10777125167,10.6343813067,1.74597978678,1.87371415638,3.05666214118,4.25015170098,5.41739602377,1.3591440864,5.39413316381,1.99728340531,2.60051373882,1.03631833906,1.84489038736,1.9309069905,1.56744850616,4.79822273514,12.7835756062,2.81226278352,1.15793203275,3.36590125907,2.27752141238,4.68127469437,1.59481418379,14.766205818,1.06002336404,6.65204120727,5.38105748636,2.45719082753,1.45977478769,3.39561043567,3.65726789804,7.60789427856,1.30899409113,2.78502832014,6.52261888982,1.33415335829,5.53287539576,6.08648717972,2.70238952964,24.091697686,5.31205610565,1.02379593935,2.18720889446,3.50000047807,1.77511737773,3.21981612108,5.87587888514,3.10180067932,1.14773303392,1.34864886003,1.91377930569,3.20029396418,1.45565018464,9.28647324064,2.00648289307,5.61450659369,1.8404706302,3.42399980241,1.30636859272,3.23095318048,6.04117832619,4.35084418735,2.8326874458,2.51704999356,8.01980751643,7.63612539848,12.1156552323,9.35598249381,5.53016445021,0.662411413467,4.82762376145,5.98143922072,4.71228717442,3.49911675114,3.15649382842,7.28404255387,2.50714283341,3.99654246428,4.6593294289,7.30404301065,4.77942039363,5.93091933672,2.98581819318,6.14331875059,9.59073371331,3.41796909143,2.44069167514,4.06647355583,2.1690331612,1.77736244543,1.31676292492,6.09822923017,3.41553265724,10.8870716579,6.14280137123,1.68252017486,8.29023270718,6.40357532329,4.68601570517,5.31474829299,5.65197642851,1.70917824452,2.59464949184,1.49625246313,6.50560937537,9.56504264345,1.35844356097,1.3096608072,3.10964475317,4.61139082571,1.50919588233,1.48144854061,4.17862950238,6.9475921965,1.61502393086,5.4657975276,1.62686018437,3.53487010124,2.14420918727,3.41725182179,2.3779426969,5.95919469671,4.69372729939,7.21201914786,2.7790797627,3.95057970601,10.8026975492,1.5662505282,7.48600879159,9.21086540122,2.41192321259,7.20546492647,2.03029495307,1.97235457665,4.73110781332,2.14894804075,3.0953030396,19.3268704886,3.64981984064,6.61976161542,21.6035270001,2.94267322267,4.57863698444,2.41747570558,1.02273859208,5.03549812523,3.3423391123]))]\n",
      "85876\n"
     ]
    }
   ],
   "source": [
    "# We will use the spark ml library for this (took a bit long to figure out)\n",
    "# prepare hashing for the data \n",
    "# HashingTF calculates a number given a  number for features for a list of words in a document\n",
    "#htf = HashingTF(numFeatures=2) # features need to match dimensions of output\n",
    "\n",
    "htf = HashingTF(numFeatures=200) \n",
    "\n",
    "hashedElements = removeSW_output.map(lambda x : htf.transform(x[1]))\n",
    "\n",
    "labels = removeSW_output.map(lambda x : 1.0 if float(x[0]) >= 2.5 else 0.0)\n",
    "#print hashedElements.top(2)\n",
    "'''\n",
    "[SparseVector(200, {1: 1.0, 32: 1.0, 45: 1.0, 90: 2.0, 94: 1.0, 124: 1.0, 126: 1.0, 135: 1.0, 138: 1.0, 185: 1.0, 199: 1.0}), SparseVector(200, {1: 2.0, 3: 1.0, 12: 2.0, 17: 1.0, 24: 1.0, 34: 1.0, 38: 1.0, 39: 1.0, 47: 2.0, 64: 1.0, 68: 2.0, 70: 1.0, 72: 1.0, 75: 1.0, 80: 1.0, 81: 1.0, 87: 1.0, 90: 1.0, 99: 1.0, 102: 1.0, 108: 1.0, 110: 1.0, 112: 1.0, 115: 1.0, 119: 1.0, 122: 1.0, 124: 1.0, 130: 1.0, 135: 1.0, 137: 1.0, 143: 1.0, 147: 1.0, 151: 1.0, 155: 1.0, 157: 1.0, 159: 2.0, 162: 1.0, 166: 1.0, 171: 1.0, 179: 1.0, 186: 1.0, 191: 1.0, 196: 1.0, 198: 1.0})]\n",
    "\n",
    "'''\n",
    "\n",
    "hashedElements.cache()\n",
    "idf = IDF().fit(hashedElements)\n",
    "tfidf = idf.transform(hashedElements)\n",
    "\"\"\"\n",
    "IDF(minDocFreq=2).fit\"\"\"\n",
    "\n",
    "\n",
    "#merge transformed data with original\n",
    "merged=  labels.zip(tfidf)\n",
    "print merged.top(2)\n",
    "print \n",
    "feature_vector  = merged.map(lambda q: LabeledPoint(q[0], q[1])   )\n",
    "print feature_vector.top(1)\n",
    "\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "print feature_vector.filter(lambda x: x.label == 1.0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training a SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error of the SVM model: 14.843\n",
      "training accuracy of the SVM model: 85.157\n",
      "\n",
      "testing error of the SVM model: 14.565\n",
      "testing accuracy of the SVM model: 85.435\n"
     ]
    }
   ],
   "source": [
    "# train a SVM classifier using 70% training data and 30% testing data from our file\n",
    "svm_training_data, svm_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "svm_model = SVMWithSGD.train(svm_training_data, iterations=500) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "svm_labelsAndPreds = svm_training_data.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_trainErr = 100* svm_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(svm_training_data.count())\n",
    "print(\"training error of the SVM model: \" + \"%.3f\" %(svm_trainErr))\n",
    "print(\"training accuracy of the SVM model: \" + \"%.3f\" %(100-svm_trainErr))\n",
    "print\n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "svm_labelsAndPredsTest = svm_testing_data.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_testErr = 100* svm_labelsAndPredsTest.filter(lambda (v, p): v != p).count() / float(svm_testing_data.count())\n",
    "print(\"testing error of the SVM model: \" + \"%.3f\" %(svm_testErr))\n",
    "print(\"testing accuracy of the SVM model: \" + \"%.3f\" %(100-svm_testErr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Training Accuracy: 84.823%\n",
      "training error of the LogisticRegressionWithLBFGS model= 15.177%\n",
      "\n",
      "Log Testing Accuracy: 84.772%\n",
      "testing error of the LogisticRegressionWithLBFGS model: 15.228%\n"
     ]
    }
   ],
   "source": [
    "# train a log_regression classifier using 70% training data and 30% testing data from our file\n",
    "\n",
    "log_training_data, log_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "log_model = LogisticRegressionWithLBFGS.train(log_training_data, iterations=500) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "log_labelsAndPreds = log_training_data.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_trainErr = 100* log_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(log_training_data.count())\n",
    "print \"Log Training Accuracy: \" + \"%.3f\" %(100-log_trainErr) + \"%\" \n",
    "print(\"training error of the LogisticRegressionWithLBFGS model= \" + \"%.3f\" % (log_trainErr)) + \"%\" \n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "log_labelsAndPredsTest = log_testing_data.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_testErr = 100* log_labelsAndPredsTest.filter(lambda (v, p): v != p).count() / float(log_testing_data.count())\n",
    "print \n",
    "print \"Log Testing Accuracy: \" + \"%.3f\" %(100-log_testErr) + \"%\" \n",
    "print(\"testing error of the LogisticRegressionWithLBFGS model: \" + \"%.3f\" %(log_testErr)) + \"%\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy: 84.638%\n",
      "Bayes Training error: 15.362%\n",
      "\n",
      "Naive Bayes Testing Accuracy: 84.251%\n",
      "Bayes Testing error: 15.749%\n"
     ]
    }
   ],
   "source": [
    "### THIS USES NAIVE BAYES CLASSIFIER\n",
    "\n",
    "\n",
    "# would be nice to figure out which division is the best\n",
    "#training_data, testing_data= feature_vector.randomSplit([0.6, 0.4])\n",
    "bayes_training_data, bayes_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "# parameters:\n",
    "lamda = 1.0\n",
    "\n",
    "# build Naive Bayes Classifier\n",
    "nbay = mllib_class.NaiveBayes.train(bayes_training_data, lamda)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bayes_labelsAndPreds = bayes_training_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "#bayes_trainErr = bayes_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(bayes_training_data.count())\n",
    "bayes_trainErr = 100.0 * bayes_labelsAndPreds.filter(lambda (x, v): x == v).count() / bayes_training_data.count()\n",
    "\n",
    "print \"Naive Bayes Training Accuracy: \" + \"%.3f\" %(bayes_trainErr) + \"%\" \n",
    "print \"Bayes Training error: \" + \"%.3f\" % (100-bayes_trainErr) + \"%\" \n",
    "\n",
    "print \n",
    "# Testing Data Accuracy\n",
    "\n",
    "bayes_labelsAndPreds = bayes_testing_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "#bayes_testErr = bayes_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(bayes_testing_data.count())\n",
    "bayes_testErr = 100.0 * bayes_labelsAndPreds.filter(lambda (x, v): x == v).count() / bayes_testing_data.count()\n",
    "\n",
    "\n",
    "print \"Naive Bayes Testing Accuracy: \" + \"%.3f\" %(bayes_testErr) + \"%\" \n",
    "print \"Bayes Testing error: \" + \"%.3f\" % (100-bayes_testErr) + \"%\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare a new data set for using our models to predict the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n",
      "('Number of datapoints in dataset to predict: ', 10000)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# PREDICTION - LETS USE OUR MODELS to predict what label we think it is (and compare it to actual values we have)\n",
    "# notes these data points are distinct from those used to train and test our models\n",
    "\n",
    "\n",
    "# first apply tokenization\n",
    "#smaller_predictData\n",
    "newdata = sc.textFile(\"smaller_predictData.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "\n",
    "# parse data for the params we want\n",
    "strippedData = newdata.map(lambda L: (L[0], float(L[4]), L[6], L[7]) if len(L) == 8 else L )\n",
    "completeData = strippedData.filter(lambda L: len(L) == 4)\n",
    "print (strippedData.count())\n",
    "print (\"Number of datapoints in dataset to predict: \",completeData.count())\n",
    "print (strippedData.count() == completeData.count())\n",
    "\n",
    "\n",
    "movieData = completeData.map(lambda x: (  x[1], tokenize(x[3]))   )\n",
    "\n",
    "#.filter(lambda x: float (x) >= 2.5).collect())\n",
    "#print movieData.filter(lambda x: float(x[1]) >= 2.5).top(10)\n",
    "#print movieData.filter(lambda x: float(x[1]) >= 2.5).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hash new tokenized data into numeric values for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LabeledPoint(1.0, (200,[2,12,20,43,45,48,54,63,69,72,84,91,92,98,101,102,104,110,111,114,117,120,125,141,147,152,154,159,169,179,191,192,196],[0.755122579278,0.628583662235,1.41232773815,0.954352233162,0.995704533594,1.51014516356,1.44190706605,1.30242092234,1.25221317465,1.33989252247,1.13641415085,1.44190706605,1.36424155148,1.35955770217,1.62566728936,1.56217733607,1.07333697734,1.09432193537,1.54562458169,1.62312986095,1.1136228964,1.2773605951,1.36855443799,1.14109800016,0.768401849161,2.95176304524,2.94390448896,1.50878800433,1.5669574599,1.50202969972,1.23521952832,3.18771971116,1.13423583974]))]\n"
     ]
    }
   ],
   "source": [
    "# HASH OUR DATA TO PREDICT\n",
    "\n",
    "htf = HashingTF(numFeatures=200) \n",
    "predict_hashedElements = movieData.map(lambda x : htf.transform(x[1]))\n",
    "\n",
    "predict_labels = movieData.map(lambda x : 1.0 if float(x[0]) >= 2.5 else 0.0)\n",
    "#print hashedElements.top(2)\n",
    "'''\n",
    "[SparseVector(200, {1: 1.0, 32: 1.0, 45: 1.0, 90: 2.0, 94: 1.0, 124: 1.0, 126: 1.0, 135: 1.0, 138: 1.0, 185: 1.0, 199: 1.0}), SparseVector(200, {1: 2.0, 3: 1.0, 12: 2.0, 17: 1.0, 24: 1.0, 34: 1.0, 38: 1.0, 39: 1.0, 47: 2.0, 64: 1.0, 68: 2.0, 70: 1.0, 72: 1.0, 75: 1.0, 80: 1.0, 81: 1.0, 87: 1.0, 90: 1.0, 99: 1.0, 102: 1.0, 108: 1.0, 110: 1.0, 112: 1.0, 115: 1.0, 119: 1.0, 122: 1.0, 124: 1.0, 130: 1.0, 135: 1.0, 137: 1.0, 143: 1.0, 147: 1.0, 151: 1.0, 155: 1.0, 157: 1.0, 159: 2.0, 162: 1.0, 166: 1.0, 171: 1.0, 179: 1.0, 186: 1.0, 191: 1.0, 196: 1.0, 198: 1.0})]\n",
    "\n",
    "'''\n",
    "\n",
    "predict_hashedElements.cache()\n",
    "pidf = IDF().fit(predict_hashedElements)\n",
    "ptfidf = pidf.transform(predict_hashedElements)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IDF(minDocFreq=2).fit\"\"\"\n",
    "\n",
    "\n",
    "#merge transformed data with original\n",
    "predict_merged=  predict_labels.zip(ptfidf)\n",
    "#print predict_merged.top(2)\n",
    "print \n",
    "featurePV  = predict_merged.map(lambda q: LabeledPoint(q[0], q[1])   )\n",
    "print featurePV.top(1)\n",
    "\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "#print featurePV.filter(lambda x: x.label == 1.0).count()\n",
    "\n",
    "# 0.8588372954"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PREDICTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Prediction Error: 12.950%\n",
      "SVM  Prediction Accuracy: 87.050%\n"
     ]
    }
   ],
   "source": [
    "# SVM PREDICTIONS\n",
    "\n",
    "#print (svm_model) \n",
    "\"\"\"\n",
    "(weights=[0.0368997186104,0.0592059564322,0.0511764523985,0.0944362393672,-0.1234655326,-0.0213073956774,0.0598834383627,0.0301296198158,0.0071253205336,-0.0773313907846,0.135198560493,0.0150622838296,0.252044696973,-0.0485074421553,0.0472279727127,-0.0602539313766,0.0113035109407,0.0384187101225,-0.188469809917,0.00267308133624,-0.0642051128667,0.0501615460127,0.185930140409,-0.0366052969578,0.056863287584,0.0493490204659,0.0300440006011,-0.0113687899148,-0.0502989129382,-0.00495112968516,0.0705056106545,0.0723744391356,-0.00780960280399,0.0648763892711,-0.0142119736707,-0.0101463323161,-0.0950565271662,-0.025407627537,0.0165633108803,0.0726917340991,-0.0543413768729,-0.11493914306,0.048542911942,-0.00490814080509,0.00183017415496,-0.0154270882776,-0.0205600111133,0.190822611363,-0.0085785906532,0.00292457229289,0.0908497832715,-0.0306247814764,-0.0272281652519,-0.00461787021553,-0.0155235065268,0.0296160453275,-0.139970211245,0.0709340007922,0.0141226375075,0.0767565407429,0.0923194359137,-0.0793072158897,-0.162151121072,-0.00582485572825,-0.027916288075,-0.0210472483295,0.00345838293048,0.013724710527,0.101103869025,-0.0297336253554,0.0279691564116,0.0689764908622,-0.13494363691,-0.0668335425458,-0.060322739572,-0.0956377298681,0.0769876814833,0.0117181091065,0.0126856313014,0.109426784166,0.0861160290819,-0.0244508540554,0.0161106487066,0.0400982188816,-0.00548897974707,-0.019190100889,0.00461539237948,0.149560544582,0.0121178683009,-0.0639882247701,0.529805130452,-0.0128682615876,0.00480636850342,-0.00410932632816,0.0351692687322,-0.0198583418872,-0.0229548568145,-0.00340860804861,0.0333486117234,-0.0330106365069,0.0412798754816,0.0537416796427,0.0374752509553,0.0180206899498,-0.00407131571665,-0.0479947909314,-0.0435645227459,-0.00546051146944,-0.0673355134481,-0.0122398316702,-0.0386454914434,0.0319355800185,-0.0641061556095,0.285542644392,-0.0755651792838,-0.0206629484963,0.023072234171,0.0700045307581,-0.0248450979802,0.0345836657542,0.043560324962,0.189977075545,-0.0267604623301,-0.134677417127,0.0549936914647,0.0470644285613,0.0516906946357,0.0294428387543,0.0647815568887,0.0564883632182,-0.105579244454,0.00852526947916,0.00525727645833,-0.0596530787731,-0.0415343964404,0.11638812774,0.0968259524311,0.208324958134,-0.00117460116655,-0.0505781712885,-0.0323338532613,-0.132213597077,-0.0048034943035,0.125680763706,0.00720316225617,0.0482061313933,-0.0819678955725,0.225878458733,-0.0487156611568,0.0450780570356,0.113355924008,-0.0311896686759,-0.0509124818306,0.036520188844,-0.0269550841676,-0.0107445622353,0.0824359871593,-0.0148080705468,0.0813362368022,0.00962483814534,0.0986313594561,0.0438001312644,0.0281565279201,0.023460809598,-0.0669994037871,-0.0285583255943,-0.0715752292781,-0.0355364452118,0.0191930116858,0.0161009483418,-0.0956563973472,0.141158315143,0.0294657105506,-0.0151626852378,0.0865943223035,-0.0415571028551,-0.0342841059177,-0.0237316978741,-0.0302115070006,-0.00182684799626,0.00168597665351,-0.0041243305443,0.0217597781446,-0.0846252980175,-0.013768106553,0.12940121048,0.0437548868609,0.0418497207055,0.123918612503,-0.0535942910674,0.0279919178596,0.0752968777908,-0.0366714491301,-0.00508041992326,0.0590876803126,-0.116625416506,-0.0194474547958,-0.0545133996421,0.023332139766,-0.014695114347], intercept=0.0)\n",
    "\"\"\"\n",
    "#print (log_model) \n",
    "#print (nbay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "svm_predictions = featurePV.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_PErr = 100* svm_predictions.filter(lambda (v, p): v != p).count() / float(featurePV.count())\n",
    "print \"SVM Prediction Error: \" + \"%.3f\" %(svm_PErr) + \"%\" \n",
    "print \"SVM  Prediction Accuracy: \" + \"%.3f\" % (100-svm_PErr) + \"%\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression Prediction Error: 13.600%\n",
      "Log Regression  Prediction Accuracy: 86.400%\n"
     ]
    }
   ],
   "source": [
    "# Log\n",
    "\n",
    "log_predictions = featurePV.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_PErr = 100* log_predictions.filter(lambda (v, p): v != p).count() / float(featurePV.count())\n",
    "print \"Log Regression Prediction Error: \" + \"%.3f\" %(log_PErr) + \"%\" \n",
    "print \"Log Regression  Prediction Accuracy: \" + \"%.3f\" % (100-log_PErr) + \"%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Prediction Error: 13.630%\n",
      "Bayes Prediction Accuracy: 86.370%\n"
     ]
    }
   ],
   "source": [
    "# Bayes\n",
    "bayes_predictions = featurePV.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "bayes_PErr = 100.0 * bayes_predictions.filter(lambda (x, v): x != v).count() / featurePV.count()\n",
    "print \"Naive Bayes Prediction Error: \" + \"%.3f\" %(bayes_PErr) + \"%\" \n",
    "print \"Bayes Prediction Accuracy: \" + \"%.3f\" % (100-bayes_PErr) + \"%\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum Features = 200, models trained with 49994 *.87 datapoints\\n\\nTraining Accuracies: \\n\\nSVM: \\nLog: \\nBayes:\\n\\nSVM Prediction Accuracy: 12.960%\\nSVM  Prediction error: 87.040%\\n\\n\\nLog Regression Prediction Accuracy: 13.380%\\nLog Regression  Prediction error: 86.620%\\n\\n\\nNaive Bayes Prediction Accuracy: 86.490%\\nBayes Prediction error: 13.510%\\n\\n\\n\\n6k - num Features:200\\n\\nTraining Accuracies: \\n\\nSVM: SVM model: 84.997 (with testing sets)\\nLog: Log Testing Accuracy: 84.292%\\nBayes: Naive Bayes Testing Accuracy: 85.145%\\n\\n\\nSVM  Prediction Accuracy: 84.290%\\nLog Regression  Prediction Accuracy: 84.280%\\nBayes Prediction Accuracy: 85.880%\\n\\n\\n\\n50k - num Features 200\\nSVM: testing accuracy of the SVM model: 85.708\\nLog:Log Testing Accuracy: 84.816%\\nBayes: Naive Bayes Testing Accuracy: 84.779%\\n\\nSVM  Prediction Accuracy: 87.100%\\nLog Regression  Prediction Accuracy: 86.490%\\nBayes Prediction Accuracy: 86.490%\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "num Features = 200, models trained with 49994 *.87 datapoints\n",
    "\n",
    "Training Accuracies: \n",
    "\n",
    "SVM: \n",
    "Log: \n",
    "Bayes:\n",
    "\n",
    "SVM Prediction Accuracy: 12.960%\n",
    "SVM  Prediction error: 87.040%\n",
    "\n",
    "\n",
    "Log Regression Prediction Accuracy: 13.380%\n",
    "Log Regression  Prediction error: 86.620%\n",
    "\n",
    "\n",
    "Naive Bayes Prediction Accuracy: 86.490%\n",
    "Bayes Prediction error: 13.510%\n",
    "\n",
    "\n",
    "\n",
    "6k - num Features:200\n",
    "\n",
    "Training Accuracies: \n",
    "\n",
    "SVM: SVM model: 84.997 (with testing sets)\n",
    "Log: Log Testing Accuracy: 84.292%\n",
    "Bayes: Naive Bayes Testing Accuracy: 85.145%\n",
    "\n",
    "\n",
    "SVM  Prediction Accuracy: 84.290%\n",
    "Log Regression  Prediction Accuracy: 84.280%\n",
    "Bayes Prediction Accuracy: 85.880%\n",
    "\n",
    "\n",
    "\n",
    "50k - num Features 200\n",
    "SVM: testing accuracy of the SVM model: 85.708\n",
    "Log:Log Testing Accuracy: 84.816%\n",
    "Bayes: Naive Bayes Testing Accuracy: 84.779%\n",
    "\n",
    "SVM  Prediction Accuracy: 87.100%\n",
    "Log Regression  Prediction Accuracy: 86.490%\n",
    "Bayes Prediction Accuracy: 86.490%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "100k - num Features 200\n",
    "SVM: testing accuracy of the SVM model: 85.435\n",
    "Log: Log Testing Accuracy: 84.772%\n",
    "Bayes: Naive Bayes Testing Accuracy: 84.251%\n",
    "\n",
    "\n",
    "SVM  Prediction Accuracy: 87.050%\n",
    "Log Regression  Prediction Accuracy: 86.400%\n",
    "Bayes Prediction Accuracy: 86.370%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
