{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Analysis Outline\n",
    "## import necessary python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pyspark' from '/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# \"transformData.py\" is this script which is intended to be run once on our data\n",
    "# transform the raw data into a rdd readable format first\n",
    "\n",
    "\n",
    "# import all necessary libraries\n",
    "import re\n",
    "import string\n",
    "from operator import add\n",
    "import os\n",
    "import sys\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import pyspark.mllib.regression as mllib_reg\n",
    "import pyspark.mllib.linalg as mllib_lalg\n",
    "import pyspark.mllib.classification as mllib_class\n",
    "import pyspark.mllib.tree as mllib_tree\n",
    "\n",
    "\n",
    "\n",
    "print (pyspark) # test to see that pyspark is up and running okay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-bbf21aaa5729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of all html tags in the data\n",
    "def strip_html_tags(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data.lower())\n",
    "\n",
    "#parse and take care of funky symbols in summary and text <- normalize to root words maybe?\n",
    "def cleanData (data):\n",
    "    data = strip_html_tags(data)\n",
    "    data =  re.sub(\"[\\t\\,\\:;\\(\\)\\\"\\'\\~\\-\\!\\?\\`]\", \"\",data, 0, 0)\n",
    "    #data =  re.sub(\"[\\.0`]\", \"\",data, 0, 0) # special case to get rid of \".0\" of scores -- \n",
    "    return data\n",
    "\n",
    "#re.sub(\"[\\.\\t\\,\\:;\\(\\)\\.]\", \" \", strip_html_tags(data.lower()), 0, 0)\n",
    "\n",
    "# clean the data for each element of each sub list\n",
    "def prepData (list_str):\n",
    "    L= []\n",
    "    for x in list_str:\n",
    "        #print x\n",
    "        L.append(cleanData(x).strip())\n",
    "    return L\n",
    "#####\n",
    "# Notes \n",
    "# headers for the project\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE THE FILE WE WANT TO USE AS OUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform Data uses /// as a separator for each eleent of data for each movie review\n",
    "# the original file for the current output.txt is a truncated version of all our data, so the end elements will be funky\n",
    "\n",
    "# output < smaller_parsed\n",
    "\n",
    "movies_txt2 = sc.textFile(\"50k_parsed.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "#movies_txt2 = sc.textFile(\"smaller_parsed.txt\").map(lambda x: (cleanData(x).split('///')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parsing out relevant fields - score and text of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49994, 50000)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# prep our data\n",
    "\n",
    "#####\n",
    "# Notes \n",
    "# headers for the project\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "# [u'productId', _ , _ , _ , u'score' [0] to get rid of .0, _  u'summary', u'text']\n",
    "\n",
    "# take the data that we care most about\n",
    "movies_new = movies_txt2.map(lambda L: (L[0], L[4][0], L[6], L[7]) if len(L) == 8 else L )\n",
    "movies_new1 = movies_new.filter(lambda L: len(L) == 4) # lets just take all the data that has been parsed correctly\n",
    "\n",
    "#print (movies_new.top(20))\n",
    "print (movies_new1.count(), movies_new.count())\n",
    "print (movies_new.count() == movies_new1.count())\n",
    "\n",
    "removeHTMLTags = movies_new1.map(prepData) # remove html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVISED STOP WORDS REMOVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, [u'zulu', u'superlative', u'depiction', u'19th', u'century', u'battle', u'rourkes', u'drift', u'british', u'outpost', u'heavily', u'outnumbered', u'seige', u'zulu', u'warriors', u'fought', u'desperately', u'almost', u'last', u'man', u'somehow', u'prevailed', u'terrifying', u'exciting', u'war', u'film', u'unlike', u'splendid', u'cast', u'word', u'diamond', u'entertainment', u'tapesmy', u'experience', u'slp', u'speed', u'vhs', u'videos', u'tracks', u'remarkably', u'well', u'usually', u'recorded', u'hifi', u'sound', u'well', u'standard', u'linear', u'format', u'zulu', u'also', u'available', u'dvd', u'also', u'recommendedzulu', u'dawn', u'1979', u'tells', u'complete', u'story', u'struggles', u'tribesmen', u'british', u'soldiers', u'commenced', u'1879', u'burt', u'lancaster', u'vhs', u'edition', u'dvd', u'editionparenthetical', u'number', u'preceding', u'title', u'1', u'10', u'viewer', u'poll', u'rating', u'found', u'film', u'resource', u'website', u'8', u'0', u'zulu', u'uk1964', u'stanley', u'baker', u'jack', u'hawkins', u'ulla', u'jacobson', u'james', u'booth', u'michael', u'caine', u'nigel', u'green', u'patrick', u'magee', u'richard', u'burton', u'narrator'])]\n"
     ]
    }
   ],
   "source": [
    "#REVISED STOP WORDS REMOVAL Approach\n",
    "\n",
    "#used the stopwords file in the virtual box instead... taking too long\n",
    "baseDir = os.path.join('../data')\n",
    "inputPath = os.path.join('cs100', 'lab3')\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    #reusing assignment code\n",
    "    \"\"\" An implementation of input string tokenization to exclude stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    splitList = re.split(split_regex, string.lower().strip())\n",
    "    \n",
    "    return [x for x in splitList if x!=\"\" and x not in stopwords]  \n",
    "\n",
    "\n",
    "\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int(x[1]), tokenize(x[3]))  )\n",
    "print removeSW_output.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing - making the vector of features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, [u'zulu', u'superlative', u'depiction', u'19th', u'century', u'battle', u'rourkes', u'drift', u'british', u'outpost', u'heavily', u'outnumbered', u'seige', u'zulu', u'warriors', u'fought', u'desperately', u'almost', u'last', u'man', u'somehow', u'prevailed', u'terrifying', u'exciting', u'war', u'film', u'unlike', u'splendid', u'cast', u'word', u'diamond', u'entertainment', u'tapesmy', u'experience', u'slp', u'speed', u'vhs', u'videos', u'tracks', u'remarkably', u'well', u'usually', u'recorded', u'hifi', u'sound', u'well', u'standard', u'linear', u'format', u'zulu', u'also', u'available', u'dvd', u'also', u'recommendedzulu', u'dawn', u'1979', u'tells', u'complete', u'story', u'struggles', u'tribesmen', u'british', u'soldiers', u'commenced', u'1879', u'burt', u'lancaster', u'vhs', u'edition', u'dvd', u'editionparenthetical', u'number', u'preceding', u'title', u'1', u'10', u'viewer', u'poll', u'rating', u'found', u'film', u'resource', u'website', u'8', u'0', u'zulu', u'uk1964', u'stanley', u'baker', u'jack', u'hawkins', u'ulla', u'jacobson', u'james', u'booth', u'michael', u'caine', u'nigel', u'green', u'patrick', u'magee', u'richard', u'burton', u'narrator'])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int(x[1]), tokenize(x[3]))  )\n",
    "print removeSW_output.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\n",
      "(10,[3,7],[1.0,2.0])\n",
      "[(1.0, SparseVector(200, {1: 1.9239, 3: 1.0044, 12: 1.2622, 17: 1.267, 24: 1.5998, 34: 1.0261, 38: 1.5115, 39: 1.0435, 47: 1.9026, 64: 1.2293, 68: 1.5112, 70: 1.3785, 72: 1.3311, 75: 1.3972, 80: 1.103, 81: 1.7388, 87: 1.144, 90: 0.7949, 99: 1.6047, 102: 1.5648, 108: 1.5172, 110: 1.1129, 112: 1.377, 115: 1.4915, 119: 1.4471, 122: 1.7743, 124: 1.1688, 130: 1.3666, 135: 0.724, 137: 1.3027, 143: 0.8674, 147: 0.7756, 151: 1.298, 155: 1.3738, 157: 1.556, 159: 3.0333, 162: 1.3837, 166: 1.4009, 171: 1.0717, 179: 1.57, 186: 0.9963, 191: 1.2454, 196: 1.2161, 198: 0.8439})), (1.0, SparseVector(200, {1: 0.962, 15: 2.1797, 19: 1.4782, 49: 1.5781, 53: 1.1388, 60: 1.0478, 61: 1.1023, 72: 1.3311, 74: 1.5207, 83: 1.598, 85: 1.6716, 101: 3.2041, 103: 0.8656, 132: 1.6989, 133: 1.6184, 173: 1.1847, 180: 1.517, 191: 1.2454}))]\n",
      "\n",
      "[LabeledPoint(1.0, (200,[0,1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,53,55,56,57,58,59,60,61,63,64,65,66,67,68,69,70,71,72,73,74,76,77,78,79,80,81,82,83,84,86,87,88,89,90,91,92,93,94,96,97,98,101,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,121,123,124,125,126,127,128,129,130,131,133,134,135,136,137,138,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,161,162,163,164,165,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],[4.10412043662,5.77183751717,3.8184212758,5.02210514134,3.38648151501,1.43816156174,7.75581671691,8.74746281285,7.61901309664,1.34045646445,1.32445976231,5.04899658364,3.14215839063,4.11285338012,1.08983008328,5.38302918557,2.53405122866,2.84024698877,7.39110964788,5.89817924437,3.75163562257,1.15191306039,3.19956738926,2.87767185489,9.56845757087,1.5534302755,5.51480996145,1.23860227985,4.53500064078,1.67281033039,2.78555193497,8.11943798258,3.07835161876,10.6910803797,1.730489109,1.84164834657,3.02297101453,4.17382431168,5.4918246431,1.35283695247,5.40269113621,1.99360886174,2.63602496013,1.0414705433,1.83185665398,1.90259980797,1.55579714051,4.73438994093,12.7429872805,2.81328398278,1.13883440315,3.3468997058,2.27442306561,4.63571094608,1.57252393431,14.5395941025,1.04778114726,6.61403759101,5.46493951352,2.45851797143,1.45224869219,3.44927304564,3.73000930091,7.55603662226,1.31413419358,2.75708739438,6.5584037598,1.33110029381,5.6652948844,6.08274483936,2.68137152178,23.891637287,5.30713303854,1.0157900921,2.20600469788,3.47752690975,1.82141788639,3.19600479554,5.88453175548,3.04889088083,1.14397000659,1.34252202766,1.90612799892,3.17961091867,1.47096823817,9.3188797667,1.99927421392,5.60432590917,1.83223148999,3.42739784552,1.30745711558,3.20412884693,6.05914871878,4.28460026765,2.79071239582,2.50500595259,7.95742539055,7.58607309279,12.1559776275,9.12082357185,5.50782860307,0.658586692313,4.93893943068,5.96586394736,4.74341775923,3.48148222807,3.12261987685,7.23567013779,2.54657770414,4.02894486509,4.67505235552,7.15219433721,4.76008853086,5.92820015532,2.93151435791,6.14082745947,9.56638935439,3.40359996374,2.42766451763,4.08626184233,2.17212782779,1.80219476614,1.30273595013,6.14844109375,3.43209839691,10.9185029002,6.07180393893,1.68643495782,8.29215229216,6.3837128821,4.65387889129,5.32086930522,5.74229496426,1.7311666158,2.59592744545,1.48332218685,6.57903141066,9.44672103348,1.3737915894,1.29445417756,3.11197346387,4.57471455438,1.51666757259,1.48464546348,4.15115227873,7.03037007599,1.62546728936,5.40076893436,1.62374113774,3.47555129281,2.14335125943,3.41721181818,2.36944796097,5.89398673809,4.75539965621,7.29382524477,2.77254871864,3.97680908337,10.9901473154,1.51703223665,7.44095254317,9.25895853641,2.39419163009,7.21946516686,2.06376440274,1.99252520328,4.79845977415,2.1195449107,3.10686055099,19.3183661362,3.73616843859,6.63648511901,21.6871071996,2.96699704519,4.63683664966,2.4322956604,1.02411006909,5.06322039177,3.30792238275]))]\n",
      "43380\n"
     ]
    }
   ],
   "source": [
    "# We will use the spark ml library for this (took a bit long to figure out)\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "\n",
    "# HashingTF calulates the 'tfid feature for us\n",
    "#htf = HashingTF(numFeatures=2) # features need to match dimensions of output\n",
    "\n",
    "\"\"\"\n",
    "data = removeSW_output.zipWithIndex()\n",
    "\n",
    "keys = removeSW_output.zipWithIndex().map(lambda data: data[1] )\n",
    "hashingTF = HashingTF().setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(numFeatures);\n",
    "tf = hashingTF.transform(keys)\n",
    "\n",
    "\n",
    "#... continue from the previous example\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF(minDocFreq=2).fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "// 4.) Create a IDFModel out of our flat vector RDD\n",
    "        IDFModel idfModel = new IDF().fit(hashedData);\n",
    "        // 5.) Create tfidf RDD\n",
    "        JavaRDD<Vector> idf = idfModel.transform(hashedData);\n",
    "        // 6.) Create Labledpoint RDD\n",
    "        JavaRDD<LabeledPoint> idfTransformed = idf.zip(tupleData).map(t -> {\n",
    "            return new LabeledPoint(t._2.label(), t._1);\n",
    "        });\n",
    "\"\"\"\n",
    "############################\n",
    "\n",
    "print \"just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\"\n",
    "print (htf.transform([\"hi\", \"boo\", \"hi\"]));\n",
    "\n",
    "htf = HashingTF(numFeatures=200) \n",
    "\n",
    "hashedElements = removeSW_output.map(lambda x : htf.transform(x[1]))\n",
    "\n",
    "labels = removeSW_output.map(lambda x : 1.0 if float(x[0]) >= 2.5 else 0.0)\n",
    "#print hashedElements.top(2)\n",
    "'''\n",
    "[SparseVector(200, {1: 1.0, 32: 1.0, 45: 1.0, 90: 2.0, 94: 1.0, 124: 1.0, 126: 1.0, 135: 1.0, 138: 1.0, 185: 1.0, 199: 1.0}), SparseVector(200, {1: 2.0, 3: 1.0, 12: 2.0, 17: 1.0, 24: 1.0, 34: 1.0, 38: 1.0, 39: 1.0, 47: 2.0, 64: 1.0, 68: 2.0, 70: 1.0, 72: 1.0, 75: 1.0, 80: 1.0, 81: 1.0, 87: 1.0, 90: 1.0, 99: 1.0, 102: 1.0, 108: 1.0, 110: 1.0, 112: 1.0, 115: 1.0, 119: 1.0, 122: 1.0, 124: 1.0, 130: 1.0, 135: 1.0, 137: 1.0, 143: 1.0, 147: 1.0, 151: 1.0, 155: 1.0, 157: 1.0, 159: 2.0, 162: 1.0, 166: 1.0, 171: 1.0, 179: 1.0, 186: 1.0, 191: 1.0, 196: 1.0, 198: 1.0})]\n",
    "\n",
    "'''\n",
    "\n",
    "hashedPoints = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1]) ] if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])])))\n",
    "#print hashedPoints.top(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hashedElements.cache()\n",
    "idf = IDF().fit(hashedElements)\n",
    "tfidf = idf.transform(hashedElements)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IDF(minDocFreq=2).fit\"\"\"\n",
    "\n",
    "\n",
    "#merge transformed data with original\n",
    "merged=  labels.zip(tfidf)\n",
    "print merged.top(2)\n",
    "print \n",
    "feature_vector  = merged.map(lambda q: LabeledPoint(q[0], q[1])   )\n",
    "print feature_vector.top(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "print feature_vector.filter(lambda x: x.label == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXPLORE DIFFERENT ML APPROACHS\n",
    "# https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.classification-module.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Trying SVM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error of the SVM model: 14.310\n",
      "training accuracy of the SVM model: 85.690\n",
      "testing error of the SVM model: 14.329\n",
      "training accuracy of the SVM model: 85.671\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, LogisticRegressionWithLBFGS \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "dataToHash = removeSW_output.map(lambda (x,y): (x, \" \".join(y)))\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=10) # features need to match dimensions of output\n",
    "\n",
    "tf = htf.transform(dataToHash)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)\n",
    "\n",
    "\n",
    "#print (tfidfIgnore.collect())\n",
    "print 123\n",
    "\n",
    "print (\"SVM: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = tfidfIgnore #.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "*/\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "\n",
    "svm_training_data, svm_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "svm_model = SVMWithSGD.train(svm_training_data, iterations=500) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "svm_labelsAndPreds = svm_training_data.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_trainErr = 100* svm_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(svm_training_data.count())\n",
    "print(\"training error of the SVM model: \" + \"%.3f\" %(svm_trainErr))\n",
    "print(\"training accuracy of the SVM model: \" + \"%.3f\" %(100-svm_trainErr))\n",
    "\n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "svm_labelsAndPredsTest = svm_testing_data.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_testErr = 100* svm_labelsAndPredsTest.filter(lambda (v, p): v != p).count() / float(svm_testing_data.count())\n",
    "print(\"testing error of the SVM model: \" + \"%.3f\" %(svm_testErr))\n",
    "print(\"training accuracy of the SVM model: \" + \"%.3f\" %(100-svm_testErr))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS , LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "log_training_data, log_testing_data= feature_vector.randomSplit([0.6, 0.4])\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "log_model = LogisticRegressionWithLBFGS.train(log_training_data, iterations=500) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "log_labelsAndPreds = log_training_data.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_trainErr = 100* log_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(log_training_data.count())\n",
    "print \"Log Training Accuracy: \" + \"%.3f\" %(100-log_trainErr) + \"%\" \n",
    "print(\"training error of the LogisticRegressionWithLBFGS model= \" + \"%.3f\" % (log_trainErr)) + \"%\" \n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "log_labelsAndPredsTest = log_testing_data.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_testErr = 100* log_labelsAndPredsTest.filter(lambda (v, p): v != p).count() / float(log_testing_data.count())\n",
    "\n",
    "print \"Log Testing Accuracy: \" + \"%.3f\" %(100-log_testErr) + \"%\" \n",
    "print(\"testing error of the LogisticRegressionWithLBFGS model: \" + \"%.3f\" %(log_testErr)) + \"%\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featurePV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-72ad681b04bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlog_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturePV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlog_PErr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mlog_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturePV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Log Regression Prediction Accuracy: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"%.3f\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_PErr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"%\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'featurePV' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Do some Predictions - Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### THIS USES NAIVE BAYES CLASSIFIER\n",
    "### with 116 elements with the first output.txt 86KB , gets 80-93%\n",
    "# tested with lots of data.... - 111MB\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=20 ) # features need to match dimensions of output\n",
    "print (\"BAYES: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "# would be nice to figure out which division is the best\n",
    "#training_data, testing_data= feature_vector.randomSplit([0.6, 0.4])\n",
    "bayes_training_data, bayes_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "# parameters:\n",
    "lamda = 1.0\n",
    "\n",
    "# build Naive Bayes Classifier\n",
    "nbay = mllib_class.NaiveBayes.train(bayes_training_data, lamda)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bayes_labelsAndPreds = bayes_training_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "#bayes_trainErr = bayes_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(bayes_training_data.count())\n",
    "bayes_trainErr = 100.0 * bayes_labelsAndPreds.filter(lambda (x, v): x == v).count() / bayes_training_data.count()\n",
    "\n",
    "print \"Naive Bayes Training Accuracy: \" + \"%.3f\" %(bayes_trainErr) + \"%\" \n",
    "print \"Bayes Training error: \" + \"%.3f\" % (100-bayes_trainErr) + \"%\" \n",
    "\n",
    "\n",
    "# Testing Data Accuracy\n",
    "\n",
    "bayes_labelsAndPreds = bayes_testing_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "#bayes_testErr = bayes_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(bayes_testing_data.count())\n",
    "bayes_testErr = 100.0 * bayes_labelsAndPreds.filter(lambda (x, v): x == v).count() / bayes_testing_data.count()\n",
    "\n",
    "\n",
    "print \"Naive Bayes Testing Accuracy: \" + \"%.3f\" %(bayes_testErr) + \"%\" \n",
    "print \"Bayes Testing error: \" + \"%.3f\" % (100-bayes_testErr) + \"%\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "99991\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# PREDICTION - LETS USE OUR MODEL \n",
    "\n",
    "\n",
    "#newdata = sc.textFile(\"smaller_predictData.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "#newdata = sc.textFile(\"100k_parsed.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "\n",
    "# parse data for the params we want\n",
    "\n",
    "strippedData = newdata.map(lambda L: (L[0], float(L[4]), L[6], L[7]) if len(L) == 8 else L )\n",
    "completeData = strippedData.filter(lambda L: len(L) == 4)\n",
    "print (strippedData.count())\n",
    "print (completeData.count())\n",
    "print (strippedData.count() == completeData.count())\n",
    "\n",
    "\n",
    "movieData = completeData.map(lambda x: (  x[1], tokenize(x[3]))   )\n",
    "\n",
    "#.filter(lambda x: float (x) >= 2.5).collect())\n",
    "#print movieData.filter(lambda x: float(x[1]) >= 2.5).top(10)\n",
    "#print movieData.filter(lambda x: float(x[1]) >= 2.5).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85876\n",
      "99991\n",
      "[LabeledPoint(1.0, [148.0]), LabeledPoint(1.0, [34.0])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# HashingTF calulates the tfid feature for us\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "\n",
    "featurePV = movieData.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "\n",
    "print featurePV.filter(lambda x: x.label == 1.0).count()\n",
    "print featurePV.count()\n",
    "\n",
    "\n",
    "print featurePV.top(2)\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "#print \"Number of Positives: \", feature_vector.filter(lambda x: x.label == 1.0).count()\n",
    "#print \"Total Number of Data Points \", feature_vector.filter(lambda x: x.label == 1.0).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sameModel = SVMModel.load(sc, output_dir)\n",
    "predictionAndLabel = test.map(lambda p: (sameModel.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(weights=[3.06841385062], intercept=0.0)\n",
      "SVM Prediction Accuracy: 14.116%\n",
      "SVM  Prediction error: 85.884%\n"
     ]
    }
   ],
   "source": [
    "# SVM PREDICTIONS\n",
    "\n",
    "print (svm_model) \n",
    "#print (log_model) \n",
    "#print (nbay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "svm_predictions = featurePV.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_PErr = 100* svm_predictions.filter(lambda (v, p): v != p).count() / float(featurePV.count())\n",
    "print \"SVM Prediction Accuracy: \" + \"%.3f\" %(svm_PErr) + \"%\" \n",
    "print \"SVM  Prediction error: \" + \"%.3f\" % (100-svm_PErr) + \"%\" \n",
    "\n",
    "featurePV = movieData.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression Prediction Accuracy: 14.116%\n",
      "Log Regression  Prediction error: 85.884%\n"
     ]
    }
   ],
   "source": [
    "# Log\n",
    "\n",
    "log_predictions = featurePV.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_PErr = 100* log_predictions.filter(lambda (v, p): v != p).count() / float(featurePV.count())\n",
    "print \"Log Regression Prediction Accuracy: \" + \"%.3f\" %(log_PErr) + \"%\" \n",
    "print \"Log Regression  Prediction error: \" + \"%.3f\" % (100-log_PErr) + \"%\"\n",
    "\n",
    "featurePV = movieData.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Prediction Accuracy: 85.884%\n",
      "Bayes Prediction error: 14.116%\n"
     ]
    }
   ],
   "source": [
    "# Bayes\n",
    "bayes_predictions = featurePV.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "bayes_PErr = 100.0 * bayes_predictions.filter(lambda (x, v): x == v).count() / featurePV.count()\n",
    "print \"Naive Bayes Prediction Accuracy: \" + \"%.3f\" %(bayes_PErr) + \"%\" \n",
    "print \"Bayes Prediction error: \" + \"%.3f\" % (100-bayes_PErr) + \"%\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "movies_txt2 = sc.textFile(\"50k_parsed.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "\n",
    "\n",
    "\n",
    "# take the data that we care most about\n",
    "movies_new = movies_txt2.map(lambda L: (L[0], L[4], L[6], L[7]) if len(L) == 8 else L )\n",
    "movies_new1 = movies_new.filter(lambda L: len(L) == 4) # lets just take all the data that has been parsed correctly\n",
    "\n",
    "#print (movies_new.top(20))\n",
    "print (movies_new1.count(), movies_new.count())\n",
    "print (movies_new.count() == movies_new1.count())\n",
    "\n",
    "removeHTMLTags = movies_new1.map(prepData) # remove html tags\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int((str(x[1])).replace(\"/\",\"\")), tokenize(x[3]))   )\n",
    "\n",
    "# We will use the spark ml library for this (took a bit long to figure out)\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "\n",
    "# HashingTF calulates the tfid feature for us\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "\n",
    "print \"just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\"\n",
    "print (htf.transform([\"hi\", \"boo\", \"hi\"]));\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "print feature_vector.filter(lambda x: x.label == 1.0).count()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
