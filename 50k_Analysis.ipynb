{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Analysis Outline\n",
    "## import necessary python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pyspark' from '/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# \"transformData.py\" is this script which is intended to be run once on our data\n",
    "# transform the raw data into a rdd readable format first\n",
    "\n",
    "# import all necessary libraries\n",
    "import re\n",
    "import string\n",
    "from operator import add\n",
    "import os\n",
    "import sys\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import pyspark.mllib.regression as mllib_reg\n",
    "import pyspark.mllib.linalg as mllib_lalg\n",
    "import pyspark.mllib.classification as mllib_class\n",
    "import pyspark.mllib.tree as mllib_tree\n",
    "\n",
    "\n",
    "\n",
    "print (pyspark) # test to see that pyspark is up and running okay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-bbf21aaa5729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of all html tags in the data\n",
    "def strip_html_tags(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data.lower())\n",
    "\n",
    "#parse and take care of funky symbols in summary and text <- normalize to root words maybe?\n",
    "def cleanData (data):\n",
    "    data = strip_html_tags(data)\n",
    "    data =  re.sub(\"[\\t\\,\\:;\\(\\)\\\"\\'\\~\\-\\!\\?\\`]\", \"\",data, 0, 0)\n",
    "    #data =  re.sub(\"[\\.0`]\", \"\",data, 0, 0) # special case to get rid of \".0\" of scores -- \n",
    "    return data\n",
    "\n",
    "#re.sub(\"[\\.\\t\\,\\:;\\(\\)\\.]\", \" \", strip_html_tags(data.lower()), 0, 0)\n",
    "\n",
    "# clean the data for each element of each sub list\n",
    "def prepData (list_str):\n",
    "    L= []\n",
    "    for x in list_str:\n",
    "        #print x\n",
    "        L.append(cleanData(x).strip())\n",
    "    return L\n",
    "#####\n",
    "# Notes \n",
    "# headers for the project\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE THE FILE WE WANT TO USE AS OUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform Data uses /// as a separator for each eleent of data for each movie review\n",
    "# the original file for the current output.txt is a truncated version of all our data, so the end elements will be funky\n",
    "\n",
    "# output < smaller_parsed\n",
    "\n",
    "movies_txt2 = sc.textFile(\"50k_parsed.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "#movies_txt2 = sc.textFile(\"smaller_parsed.txt\").map(lambda x: (cleanData(x).split('///')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parsing out relevant fields - score and text of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49994, 50000)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# prep our data\n",
    "\n",
    "#####\n",
    "# Notes \n",
    "# headers for the project\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "# [u'productId', _ , _ , _ , u'score' [0] to get rid of .0, _  u'summary', u'text']\n",
    "\n",
    "# take the data that we care most about\n",
    "movies_new = movies_txt2.map(lambda L: (L[0], L[4][0], L[6], L[7]) if len(L) == 8 else L )\n",
    "movies_new1 = movies_new.filter(lambda L: len(L) == 4) # lets just take all the data that has been parsed correctly\n",
    "\n",
    "#print (movies_new.top(20))\n",
    "print (movies_new1.count(), movies_new.count())\n",
    "print (movies_new.count() == movies_new1.count())\n",
    "\n",
    "removeHTMLTags = movies_new1.map(prepData) # remove html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVISED STOP WORDS REMOVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, [u'zulu', u'superlative', u'depiction', u'19th', u'century', u'battle', u'rourkes', u'drift', u'british', u'outpost', u'heavily', u'outnumbered', u'seige', u'zulu', u'warriors', u'fought', u'desperately', u'almost', u'last', u'man', u'somehow', u'prevailed', u'terrifying', u'exciting', u'war', u'film', u'unlike', u'splendid', u'cast', u'word', u'diamond', u'entertainment', u'tapesmy', u'experience', u'slp', u'speed', u'vhs', u'videos', u'tracks', u'remarkably', u'well', u'usually', u'recorded', u'hifi', u'sound', u'well', u'standard', u'linear', u'format', u'zulu', u'also', u'available', u'dvd', u'also', u'recommendedzulu', u'dawn', u'1979', u'tells', u'complete', u'story', u'struggles', u'tribesmen', u'british', u'soldiers', u'commenced', u'1879', u'burt', u'lancaster', u'vhs', u'edition', u'dvd', u'editionparenthetical', u'number', u'preceding', u'title', u'1', u'10', u'viewer', u'poll', u'rating', u'found', u'film', u'resource', u'website', u'8', u'0', u'zulu', u'uk1964', u'stanley', u'baker', u'jack', u'hawkins', u'ulla', u'jacobson', u'james', u'booth', u'michael', u'caine', u'nigel', u'green', u'patrick', u'magee', u'richard', u'burton', u'narrator'])]\n"
     ]
    }
   ],
   "source": [
    "#REVISED STOP WORDS REMOVAL Approach\n",
    "\n",
    "#used the stopwords file in the virtual box instead... taking too long\n",
    "baseDir = os.path.join('../data')\n",
    "inputPath = os.path.join('cs100', 'lab3')\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    #reusing assignment code\n",
    "    \"\"\" An implementation of input string tokenization to exclude stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    splitList = re.split(split_regex, string.lower().strip())\n",
    "    \n",
    "    return [x for x in splitList if x!=\"\" and x not in stopwords]  \n",
    "\n",
    "\n",
    "\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int(x[1]), tokenize(x[3]))  )\n",
    "print removeSW_output.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing - making the vector of features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, [u'zulu', u'superlative', u'depiction', u'19th', u'century', u'battle', u'rourkes', u'drift', u'british', u'outpost', u'heavily', u'outnumbered', u'seige', u'zulu', u'warriors', u'fought', u'desperately', u'almost', u'last', u'man', u'somehow', u'prevailed', u'terrifying', u'exciting', u'war', u'film', u'unlike', u'splendid', u'cast', u'word', u'diamond', u'entertainment', u'tapesmy', u'experience', u'slp', u'speed', u'vhs', u'videos', u'tracks', u'remarkably', u'well', u'usually', u'recorded', u'hifi', u'sound', u'well', u'standard', u'linear', u'format', u'zulu', u'also', u'available', u'dvd', u'also', u'recommendedzulu', u'dawn', u'1979', u'tells', u'complete', u'story', u'struggles', u'tribesmen', u'british', u'soldiers', u'commenced', u'1879', u'burt', u'lancaster', u'vhs', u'edition', u'dvd', u'editionparenthetical', u'number', u'preceding', u'title', u'1', u'10', u'viewer', u'poll', u'rating', u'found', u'film', u'resource', u'website', u'8', u'0', u'zulu', u'uk1964', u'stanley', u'baker', u'jack', u'hawkins', u'ulla', u'jacobson', u'james', u'booth', u'michael', u'caine', u'nigel', u'green', u'patrick', u'magee', u'richard', u'burton', u'narrator'])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int(x[1]), tokenize(x[3]))  )\n",
    "print removeSW_output.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\n",
      "(20,[3,7],[1.0,2.0])\n",
      "[(1.0, SparseVector(200, {2: 0.7637, 9: 1.5238, 12: 1.2622, 18: 1.4201, 35: 3.0546, 39: 1.0435, 43: 0.9968, 56: 1.1372, 57: 1.5452, 60: 1.0478, 74: 3.0414, 78: 0.7582, 80: 1.103, 87: 1.144, 103: 2.5968, 113: 0.3293, 115: 1.4915, 120: 1.2875, 121: 0.8489, 124: 1.1688, 129: 1.2282, 133: 1.6184, 143: 1.7348, 144: 3.3729, 147: 0.7756, 156: 1.2945, 162: 1.3837, 169: 1.6237, 172: 5.1258, 198: 0.8439})), (1.0, SparseVector(200, {0: 1.368, 1: 1.9239, 2: 0.7637, 3: 1.0044, 4: 1.1288, 7: 1.2496, 12: 1.2622, 13: 1.5711, 14: 1.371, 15: 3.2695, 20: 4.4236, 22: 2.3038, 24: 1.5998, 27: 1.5534, 28: 1.3787, 29: 1.2386, 33: 1.1599, 35: 1.5273, 38: 1.5115, 39: 2.0869, 42: 1.3507, 43: 0.9968, 44: 1.318, 45: 1.0415, 47: 2.8539, 52: 3.6637, 53: 4.5553, 54: 1.5312, 57: 3.0905, 58: 1.5725, 59: 1.454, 60: 3.1433, 61: 1.1023, 63: 1.3662, 64: 1.2293, 66: 0.8623, 70: 1.3785, 71: 2.6234, 72: 1.3311, 74: 1.5207, 76: 1.3407, 78: 0.7582, 79: 1.0158, 80: 6.618, 88: 1.3425, 90: 0.7949, 97: 5.1411, 99: 1.6047, 100: 1.4483, 101: 6.4083, 103: 0.8656, 105: 2.7907, 106: 1.2525, 108: 1.5172, 110: 2.2257, 111: 3.0403, 116: 4.7434, 117: 1.1605, 121: 0.8489, 122: 3.5487, 123: 1.343, 124: 1.1688, 127: 1.1856, 128: 1.4658, 130: 1.3666, 131: 3.4036, 133: 0.8092, 134: 1.3621, 135: 1.4481, 138: 1.5371, 140: 3.2493, 141: 1.144, 143: 0.8674, 148: 1.7736, 149: 1.9141, 157: 3.112, 158: 7.6245, 159: 1.5167, 161: 2.9693, 163: 1.1717, 165: 1.8003, 166: 1.4009, 171: 3.215, 173: 1.1847, 175: 3.1703, 176: 2.9175, 185: 3.0956, 186: 3.9851, 188: 1.0598, 190: 1.7562, 191: 2.4908, 194: 1.4835, 196: 1.2161}))]\n",
      "\n",
      "[LabeledPoint(1.0, (200,[0,1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,53,55,56,57,58,59,60,61,63,64,65,66,67,68,69,70,71,72,73,74,76,77,78,79,80,81,82,83,84,86,87,88,89,90,91,92,93,94,96,97,98,101,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,121,123,124,125,126,127,128,129,130,131,133,134,135,136,137,138,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,161,162,163,164,165,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],[4.10412043662,5.77183751717,3.8184212758,5.02210514134,3.38648151501,1.43816156174,7.75581671691,8.74746281285,7.61901309664,1.34045646445,1.32445976231,5.04899658364,3.14215839063,4.11285338012,1.08983008328,5.38302918557,2.53405122866,2.84024698877,7.39110964788,5.89817924437,3.75163562257,1.15191306039,3.19956738926,2.87767185489,9.56845757087,1.5534302755,5.51480996145,1.23860227985,4.53500064078,1.67281033039,2.78555193497,8.11943798258,3.07835161876,10.6910803797,1.730489109,1.84164834657,3.02297101453,4.17382431168,5.4918246431,1.35283695247,5.40269113621,1.99360886174,2.63602496013,1.0414705433,1.83185665398,1.90259980797,1.55579714051,4.73438994093,12.7429872805,2.81328398278,1.13883440315,3.3468997058,2.27442306561,4.63571094608,1.57252393431,14.5395941025,1.04778114726,6.61403759101,5.46493951352,2.45851797143,1.45224869219,3.44927304564,3.73000930091,7.55603662226,1.31413419358,2.75708739438,6.5584037598,1.33110029381,5.6652948844,6.08274483936,2.68137152178,23.891637287,5.30713303854,1.0157900921,2.20600469788,3.47752690975,1.82141788639,3.19600479554,5.88453175548,3.04889088083,1.14397000659,1.34252202766,1.90612799892,3.17961091867,1.47096823817,9.3188797667,1.99927421392,5.60432590917,1.83223148999,3.42739784552,1.30745711558,3.20412884693,6.05914871878,4.28460026765,2.79071239582,2.50500595259,7.95742539055,7.58607309279,12.1559776275,9.12082357185,5.50782860307,0.658586692313,4.93893943068,5.96586394736,4.74341775923,3.48148222807,3.12261987685,7.23567013779,2.54657770414,4.02894486509,4.67505235552,7.15219433721,4.76008853086,5.92820015532,2.93151435791,6.14082745947,9.56638935439,3.40359996374,2.42766451763,4.08626184233,2.17212782779,1.80219476614,1.30273595013,6.14844109375,3.43209839691,10.9185029002,6.07180393893,1.68643495782,8.29215229216,6.3837128821,4.65387889129,5.32086930522,5.74229496426,1.7311666158,2.59592744545,1.48332218685,6.57903141066,9.44672103348,1.3737915894,1.29445417756,3.11197346387,4.57471455438,1.51666757259,1.48464546348,4.15115227873,7.03037007599,1.62546728936,5.40076893436,1.62374113774,3.47555129281,2.14335125943,3.41721181818,2.36944796097,5.89398673809,4.75539965621,7.29382524477,2.77254871864,3.97680908337,10.9901473154,1.51703223665,7.44095254317,9.25895853641,2.39419163009,7.21946516686,2.06376440274,1.99252520328,4.79845977415,2.1195449107,3.10686055099,19.3183661362,3.73616843859,6.63648511901,21.6871071996,2.96699704519,4.63683664966,2.4322956604,1.02411006909,5.06322039177,3.30792238275]))]\n",
      "43380\n"
     ]
    }
   ],
   "source": [
    "# We will use the spark ml library for this (took a bit long to figure out)\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "\n",
    "# HashingTF calulates the 'tfid feature for us\n",
    "#htf = HashingTF(numFeatures=2) # features need to match dimensions of output\n",
    "\n",
    "\"\"\"\n",
    "data = removeSW_output.zipWithIndex()\n",
    "\n",
    "keys = removeSW_output.zipWithIndex().map(lambda data: data[1] )\n",
    "hashingTF = HashingTF().setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(numFeatures);\n",
    "tf = hashingTF.transform(keys)\n",
    "\n",
    "\n",
    "#... continue from the previous example\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF(minDocFreq=2).fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "// 4.) Create a IDFModel out of our flat vector RDD\n",
    "        IDFModel idfModel = new IDF().fit(hashedData);\n",
    "        // 5.) Create tfidf RDD\n",
    "        JavaRDD<Vector> idf = idfModel.transform(hashedData);\n",
    "        // 6.) Create Labledpoint RDD\n",
    "        JavaRDD<LabeledPoint> idfTransformed = idf.zip(tupleData).map(t -> {\n",
    "            return new LabeledPoint(t._2.label(), t._1);\n",
    "        });\n",
    "\"\"\"\n",
    "############################\n",
    "\n",
    "print \"just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\"\n",
    "print (htf.transform([\"hi\", \"boo\", \"hi\"]));\n",
    "\n",
    "htf = HashingTF(numFeatures=200) \n",
    "\n",
    "hashedElements = removeSW_output.map(lambda x : htf.transform(x[1]))\n",
    "\n",
    "labels = removeSW_output.map(lambda x : 1.0 if float(x[0]) >= 2.5 else 0.0)\n",
    "#print hashedElements.top(2)\n",
    "'''\n",
    "[SparseVector(200, {1: 1.0, 32: 1.0, 45: 1.0, 90: 2.0, 94: 1.0, 124: 1.0, 126: 1.0, 135: 1.0, 138: 1.0, 185: 1.0, 199: 1.0}), SparseVector(200, {1: 2.0, 3: 1.0, 12: 2.0, 17: 1.0, 24: 1.0, 34: 1.0, 38: 1.0, 39: 1.0, 47: 2.0, 64: 1.0, 68: 2.0, 70: 1.0, 72: 1.0, 75: 1.0, 80: 1.0, 81: 1.0, 87: 1.0, 90: 1.0, 99: 1.0, 102: 1.0, 108: 1.0, 110: 1.0, 112: 1.0, 115: 1.0, 119: 1.0, 122: 1.0, 124: 1.0, 130: 1.0, 135: 1.0, 137: 1.0, 143: 1.0, 147: 1.0, 151: 1.0, 155: 1.0, 157: 1.0, 159: 2.0, 162: 1.0, 166: 1.0, 171: 1.0, 179: 1.0, 186: 1.0, 191: 1.0, 196: 1.0, 198: 1.0})]\n",
    "\n",
    "'''\n",
    "\n",
    "hashedPoints = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1]) ] if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])])))\n",
    "#print hashedPoints.top(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hashedElements.cache()\n",
    "idf = IDF().fit(hashedElements)\n",
    "tfidf = idf.transform(hashedElements)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IDF(minDocFreq=2).fit\"\"\"\n",
    "\n",
    "\n",
    "#merge transformed data with original\n",
    "merged=  labels.zip(tfidf)\n",
    "print merged.top(2)\n",
    "print \n",
    "feature_vector  = merged.map(lambda q: LabeledPoint(q[0], q[1])   )\n",
    "print feature_vector.top(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "print feature_vector.filter(lambda x: x.label == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXPLORE DIFFERENT ML APPROACHS\n",
    "# https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.classification-module.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Trying SVM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error of the SVM model: 14.490\n",
      "training accuracy of the SVM model: 85.510\n",
      "testing error of the SVM model: 14.147\n",
      "training accuracy of the SVM model: 85.853\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, LogisticRegressionWithLBFGS \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "dataToHash = removeSW_output.map(lambda (x,y): (x, \" \".join(y)))\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=10) # features need to match dimensions of output\n",
    "\n",
    "tf = htf.transform(dataToHash)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)\n",
    "\n",
    "\n",
    "#print (tfidfIgnore.collect())\n",
    "print 123\n",
    "\n",
    "print (\"SVM: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = tfidfIgnore #.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "*/\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "\n",
    "svm_training_data, svm_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "svm_model = SVMWithSGD.train(svm_training_data, iterations=500) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "svm_labelsAndPreds = svm_training_data.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_trainErr = 100* svm_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(svm_training_data.count())\n",
    "print(\"training error of the SVM model: \" + \"%.3f\" %(svm_trainErr))\n",
    "print(\"training accuracy of the SVM model: \" + \"%.3f\" %(100-svm_trainErr))\n",
    "\n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "svm_labelsAndPredsTest = svm_testing_data.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_testErr = 100* svm_labelsAndPredsTest.filter(lambda (v, p): v != p).count() / float(svm_testing_data.count())\n",
    "print(\"testing error of the SVM model: \" + \"%.3f\" %(svm_testErr))\n",
    "print(\"training accuracy of the SVM model: \" + \"%.3f\" %(100-svm_testErr))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Training Accuracy: 85.156%\n",
      "training error of the LogisticRegressionWithLBFGS model= 14.844%\n",
      "Log Testing Accuracy: 84.922%\n",
      "testing error of the LogisticRegressionWithLBFGS model: 15.078%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS , LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "log_training_data, log_testing_data= feature_vector.randomSplit([0.6, 0.4])\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "log_model = LogisticRegressionWithLBFGS.train(log_training_data, iterations=500) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "log_labelsAndPreds = log_training_data.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_trainErr = 100* log_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(log_training_data.count())\n",
    "print \"Log Training Accuracy: \" + \"%.3f\" %(100-log_trainErr) + \"%\" \n",
    "print(\"training error of the LogisticRegressionWithLBFGS model= \" + \"%.3f\" % (log_trainErr)) + \"%\" \n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "log_labelsAndPredsTest = log_testing_data.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_testErr = 100* log_labelsAndPredsTest.filter(lambda (v, p): v != p).count() / float(log_testing_data.count())\n",
    "\n",
    "print \"Log Testing Accuracy: \" + \"%.3f\" %(100-log_testErr) + \"%\" \n",
    "print(\"testing error of the LogisticRegressionWithLBFGS model: \" + \"%.3f\" %(log_testErr)) + \"%\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Do some Predictions - Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy: 85.124%\n",
      "Bayes Training error: 14.876%\n",
      "Naive Bayes Testing Accuracy: 85.464%\n",
      "Bayes Testing error: 14.536%\n"
     ]
    }
   ],
   "source": [
    "### THIS USES NAIVE BAYES CLASSIFIER\n",
    "### with 116 elements with the first output.txt 86KB , gets 80-93%\n",
    "# tested with lots of data.... - 111MB\n",
    "\n",
    "\n",
    "\n",
    "# would be nice to figure out which division is the best\n",
    "#training_data, testing_data= feature_vector.randomSplit([0.6, 0.4])\n",
    "bayes_training_data, bayes_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "# parameters:\n",
    "lamda = 1.0\n",
    "\n",
    "# build Naive Bayes Classifier\n",
    "nbay = mllib_class.NaiveBayes.train(bayes_training_data, lamda)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bayes_labelsAndPreds = bayes_training_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "#bayes_trainErr = bayes_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(bayes_training_data.count())\n",
    "bayes_trainErr = 100.0 * bayes_labelsAndPreds.filter(lambda (x, v): x == v).count() / bayes_training_data.count()\n",
    "\n",
    "print \"Naive Bayes Training Accuracy: \" + \"%.3f\" %(bayes_trainErr) + \"%\" \n",
    "print \"Bayes Training error: \" + \"%.3f\" % (100-bayes_trainErr) + \"%\" \n",
    "\n",
    "\n",
    "# Testing Data Accuracy\n",
    "\n",
    "bayes_labelsAndPreds = bayes_testing_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "#bayes_testErr = bayes_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(bayes_testing_data.count())\n",
    "bayes_testErr = 100.0 * bayes_labelsAndPreds.filter(lambda (x, v): x == v).count() / bayes_testing_data.count()\n",
    "\n",
    "\n",
    "print \"Naive Bayes Testing Accuracy: \" + \"%.3f\" %(bayes_testErr) + \"%\" \n",
    "print \"Bayes Testing error: \" + \"%.3f\" % (100-bayes_testErr) + \"%\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "99991\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# PREDICTION - LETS USE OUR MODEL \n",
    "\n",
    "\n",
    "# first apply tokenization\n",
    "\n",
    "#newdata = sc.textFile(\"smaller_predictData.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "#newdata = sc.textFile(\"100k_parsed.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "\n",
    "# parse data for the params we want\n",
    "\n",
    "strippedData = newdata.map(lambda L: (L[0], float(L[4]), L[6], L[7]) if len(L) == 8 else L )\n",
    "completeData = strippedData.filter(lambda L: len(L) == 4)\n",
    "print (strippedData.count())\n",
    "print (completeData.count())\n",
    "print (strippedData.count() == completeData.count())\n",
    "\n",
    "\n",
    "movieData = completeData.map(lambda x: (  x[1], tokenize(x[3]))   )\n",
    "\n",
    "#.filter(lambda x: float (x) >= 2.5).collect())\n",
    "#print movieData.filter(lambda x: float(x[1]) >= 2.5).top(10)\n",
    "#print movieData.filter(lambda x: float(x[1]) >= 2.5).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, SparseVector(200, {0: 1.374, 2: 0.7789, 5: 1.432, 9: 1.5184, 12: 0.6387, 20: 1.4818, 21: 1.2526, 22: 1.1598, 25: 1.4449, 29: 1.2381, 39: 1.0625, 44: 1.3003, 47: 0.9655, 50: 1.1621, 66: 0.8489, 68: 0.7608, 76: 1.3512, 78: 2.2766, 80: 1.0936, 88: 1.3486, 90: 2.4002, 109: 1.5145, 111: 1.5593, 113: 1.3248, 118: 1.5782, 121: 0.8357, 135: 0.723, 141: 1.1385, 151: 1.2973, 170: 1.1783, 185: 1.0151, 186: 1.9724, 188: 1.0745})), (1.0, SparseVector(200, {14: 1.3862, 27: 1.5589, 28: 1.3668, 36: 1.746, 43: 0.9986, 64: 4.9144, 68: 0.7608, 88: 2.6973, 90: 0.8001, 113: 0.3312, 121: 0.8357, 124: 1.1648, 125: 1.4608, 127: 1.1862, 129: 1.2287, 135: 0.723, 146: 3.2018, 147: 0.781, 172: 1.7086, 185: 1.0151, 190: 1.757}))]\n",
      "\n",
      "[LabeledPoint(1.0, (200,[12,18,44,47,50,66,69,73,85,88,100,104,147,160,170,186,189],[0.638654752521,1.40327913655,1.30025686941,0.965453495252,2.32428647386,0.848902608918,1.30899409113,1.38321884894,1.67144610155,1.34864886003,1.43974172494,1.08771104684,1.56200523506,1.18007862303,1.17829003375,0.986177288327,1.5476515198]))]\n",
      "85876\n"
     ]
    }
   ],
   "source": [
    "# HASH OUR DATA TO PREDICT\n",
    "\n",
    "#htf = HashingTF(numFeatures=200) \n",
    "predict_hashedElements = movieData.map(lambda x : htf.transform(x[1]))\n",
    "\n",
    "predict_labels = movieData.map(lambda x : 1.0 if float(x[0]) >= 2.5 else 0.0)\n",
    "#print hashedElements.top(2)\n",
    "'''\n",
    "[SparseVector(200, {1: 1.0, 32: 1.0, 45: 1.0, 90: 2.0, 94: 1.0, 124: 1.0, 126: 1.0, 135: 1.0, 138: 1.0, 185: 1.0, 199: 1.0}), SparseVector(200, {1: 2.0, 3: 1.0, 12: 2.0, 17: 1.0, 24: 1.0, 34: 1.0, 38: 1.0, 39: 1.0, 47: 2.0, 64: 1.0, 68: 2.0, 70: 1.0, 72: 1.0, 75: 1.0, 80: 1.0, 81: 1.0, 87: 1.0, 90: 1.0, 99: 1.0, 102: 1.0, 108: 1.0, 110: 1.0, 112: 1.0, 115: 1.0, 119: 1.0, 122: 1.0, 124: 1.0, 130: 1.0, 135: 1.0, 137: 1.0, 143: 1.0, 147: 1.0, 151: 1.0, 155: 1.0, 157: 1.0, 159: 2.0, 162: 1.0, 166: 1.0, 171: 1.0, 179: 1.0, 186: 1.0, 191: 1.0, 196: 1.0, 198: 1.0})]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "predict_hashedElements.cache()\n",
    "pidf = IDF().fit(predict_hashedElements)\n",
    "ptfidf = pidf.transform(predict_hashedElements)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IDF(minDocFreq=2).fit\"\"\"\n",
    "\n",
    "\n",
    "#merge transformed data with original\n",
    "predict_merged=  predict_labels.zip(ptfidf)\n",
    "print predict_merged.top(2)\n",
    "print \n",
    "featurePV  = predict_merged.map(lambda q: LabeledPoint(q[0], q[1])   )\n",
    "print featurePV.top(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "print featurePV.filter(lambda x: x.label == 1.0).count()\n",
    "\n",
    "# 0.8588372954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\n# HashingTF calulates the tfid feature for us\\nhtf = HashingTF(numFeatures=1) # features need to match dimensions of output\\n\\nfeaturePV = movieData.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\\n\\nprint featurePV.filter(lambda x: x.label == 1.0).count()\\nprint featurePV.count()\\n\\n\\nprint featurePV.top(2)\\n#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\\n#print \"Number of Positives: \", feature_vector.filter(lambda x: x.label == 1.0).count()\\n#print \"Total Number of Data Points \", feature_vector.filter(lambda x: x.label == 1.0).count()\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''   \n",
    "# HashingTF calulates the tfid feature for us\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "\n",
    "featurePV = movieData.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "\n",
    "print featurePV.filter(lambda x: x.label == 1.0).count()\n",
    "print featurePV.count()\n",
    "\n",
    "\n",
    "print featurePV.top(2)\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "#print \"Number of Positives: \", feature_vector.filter(lambda x: x.label == 1.0).count()\n",
    "#print \"Total Number of Data Points \", feature_vector.filter(lambda x: x.label == 1.0).count()\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "sameModel = SVMModel.load(sc, output_dir)\n",
    "predictionAndLabel = test.map(lambda p: (sameModel.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(weights=[0.0368997186104,0.0592059564322,0.0511764523985,0.0944362393672,-0.1234655326,-0.0213073956774,0.0598834383627,0.0301296198158,0.0071253205336,-0.0773313907846,0.135198560493,0.0150622838296,0.252044696973,-0.0485074421553,0.0472279727127,-0.0602539313766,0.0113035109407,0.0384187101225,-0.188469809917,0.00267308133624,-0.0642051128667,0.0501615460127,0.185930140409,-0.0366052969578,0.056863287584,0.0493490204659,0.0300440006011,-0.0113687899148,-0.0502989129382,-0.00495112968516,0.0705056106545,0.0723744391356,-0.00780960280399,0.0648763892711,-0.0142119736707,-0.0101463323161,-0.0950565271662,-0.025407627537,0.0165633108803,0.0726917340991,-0.0543413768729,-0.11493914306,0.048542911942,-0.00490814080509,0.00183017415496,-0.0154270882776,-0.0205600111133,0.190822611363,-0.0085785906532,0.00292457229289,0.0908497832715,-0.0306247814764,-0.0272281652519,-0.00461787021553,-0.0155235065268,0.0296160453275,-0.139970211245,0.0709340007922,0.0141226375075,0.0767565407429,0.0923194359137,-0.0793072158897,-0.162151121072,-0.00582485572825,-0.027916288075,-0.0210472483295,0.00345838293048,0.013724710527,0.101103869025,-0.0297336253554,0.0279691564116,0.0689764908622,-0.13494363691,-0.0668335425458,-0.060322739572,-0.0956377298681,0.0769876814833,0.0117181091065,0.0126856313014,0.109426784166,0.0861160290819,-0.0244508540554,0.0161106487066,0.0400982188816,-0.00548897974707,-0.019190100889,0.00461539237948,0.149560544582,0.0121178683009,-0.0639882247701,0.529805130452,-0.0128682615876,0.00480636850342,-0.00410932632816,0.0351692687322,-0.0198583418872,-0.0229548568145,-0.00340860804861,0.0333486117234,-0.0330106365069,0.0412798754816,0.0537416796427,0.0374752509553,0.0180206899498,-0.00407131571665,-0.0479947909314,-0.0435645227459,-0.00546051146944,-0.0673355134481,-0.0122398316702,-0.0386454914434,0.0319355800185,-0.0641061556095,0.285542644392,-0.0755651792838,-0.0206629484963,0.023072234171,0.0700045307581,-0.0248450979802,0.0345836657542,0.043560324962,0.189977075545,-0.0267604623301,-0.134677417127,0.0549936914647,0.0470644285613,0.0516906946357,0.0294428387543,0.0647815568887,0.0564883632182,-0.105579244454,0.00852526947916,0.00525727645833,-0.0596530787731,-0.0415343964404,0.11638812774,0.0968259524311,0.208324958134,-0.00117460116655,-0.0505781712885,-0.0323338532613,-0.132213597077,-0.0048034943035,0.125680763706,0.00720316225617,0.0482061313933,-0.0819678955725,0.225878458733,-0.0487156611568,0.0450780570356,0.113355924008,-0.0311896686759,-0.0509124818306,0.036520188844,-0.0269550841676,-0.0107445622353,0.0824359871593,-0.0148080705468,0.0813362368022,0.00962483814534,0.0986313594561,0.0438001312644,0.0281565279201,0.023460809598,-0.0669994037871,-0.0285583255943,-0.0715752292781,-0.0355364452118,0.0191930116858,0.0161009483418,-0.0956563973472,0.141158315143,0.0294657105506,-0.0151626852378,0.0865943223035,-0.0415571028551,-0.0342841059177,-0.0237316978741,-0.0302115070006,-0.00182684799626,0.00168597665351,-0.0041243305443,0.0217597781446,-0.0846252980175,-0.013768106553,0.12940121048,0.0437548868609,0.0418497207055,0.123918612503,-0.0535942910674,0.0279919178596,0.0752968777908,-0.0366714491301,-0.00508041992326,0.0590876803126,-0.116625416506,-0.0194474547958,-0.0545133996421,0.023332139766,-0.014695114347], intercept=0.0)\n",
      "SVM Prediction Accuracy: 15.022%\n",
      "SVM  Prediction error: 84.978%\n"
     ]
    }
   ],
   "source": [
    "# SVM PREDICTIONS\n",
    "\n",
    "print (svm_model) \n",
    "\"\"\"\n",
    "(weights=[0.0368997186104,0.0592059564322,0.0511764523985,0.0944362393672,-0.1234655326,-0.0213073956774,0.0598834383627,0.0301296198158,0.0071253205336,-0.0773313907846,0.135198560493,0.0150622838296,0.252044696973,-0.0485074421553,0.0472279727127,-0.0602539313766,0.0113035109407,0.0384187101225,-0.188469809917,0.00267308133624,-0.0642051128667,0.0501615460127,0.185930140409,-0.0366052969578,0.056863287584,0.0493490204659,0.0300440006011,-0.0113687899148,-0.0502989129382,-0.00495112968516,0.0705056106545,0.0723744391356,-0.00780960280399,0.0648763892711,-0.0142119736707,-0.0101463323161,-0.0950565271662,-0.025407627537,0.0165633108803,0.0726917340991,-0.0543413768729,-0.11493914306,0.048542911942,-0.00490814080509,0.00183017415496,-0.0154270882776,-0.0205600111133,0.190822611363,-0.0085785906532,0.00292457229289,0.0908497832715,-0.0306247814764,-0.0272281652519,-0.00461787021553,-0.0155235065268,0.0296160453275,-0.139970211245,0.0709340007922,0.0141226375075,0.0767565407429,0.0923194359137,-0.0793072158897,-0.162151121072,-0.00582485572825,-0.027916288075,-0.0210472483295,0.00345838293048,0.013724710527,0.101103869025,-0.0297336253554,0.0279691564116,0.0689764908622,-0.13494363691,-0.0668335425458,-0.060322739572,-0.0956377298681,0.0769876814833,0.0117181091065,0.0126856313014,0.109426784166,0.0861160290819,-0.0244508540554,0.0161106487066,0.0400982188816,-0.00548897974707,-0.019190100889,0.00461539237948,0.149560544582,0.0121178683009,-0.0639882247701,0.529805130452,-0.0128682615876,0.00480636850342,-0.00410932632816,0.0351692687322,-0.0198583418872,-0.0229548568145,-0.00340860804861,0.0333486117234,-0.0330106365069,0.0412798754816,0.0537416796427,0.0374752509553,0.0180206899498,-0.00407131571665,-0.0479947909314,-0.0435645227459,-0.00546051146944,-0.0673355134481,-0.0122398316702,-0.0386454914434,0.0319355800185,-0.0641061556095,0.285542644392,-0.0755651792838,-0.0206629484963,0.023072234171,0.0700045307581,-0.0248450979802,0.0345836657542,0.043560324962,0.189977075545,-0.0267604623301,-0.134677417127,0.0549936914647,0.0470644285613,0.0516906946357,0.0294428387543,0.0647815568887,0.0564883632182,-0.105579244454,0.00852526947916,0.00525727645833,-0.0596530787731,-0.0415343964404,0.11638812774,0.0968259524311,0.208324958134,-0.00117460116655,-0.0505781712885,-0.0323338532613,-0.132213597077,-0.0048034943035,0.125680763706,0.00720316225617,0.0482061313933,-0.0819678955725,0.225878458733,-0.0487156611568,0.0450780570356,0.113355924008,-0.0311896686759,-0.0509124818306,0.036520188844,-0.0269550841676,-0.0107445622353,0.0824359871593,-0.0148080705468,0.0813362368022,0.00962483814534,0.0986313594561,0.0438001312644,0.0281565279201,0.023460809598,-0.0669994037871,-0.0285583255943,-0.0715752292781,-0.0355364452118,0.0191930116858,0.0161009483418,-0.0956563973472,0.141158315143,0.0294657105506,-0.0151626852378,0.0865943223035,-0.0415571028551,-0.0342841059177,-0.0237316978741,-0.0302115070006,-0.00182684799626,0.00168597665351,-0.0041243305443,0.0217597781446,-0.0846252980175,-0.013768106553,0.12940121048,0.0437548868609,0.0418497207055,0.123918612503,-0.0535942910674,0.0279919178596,0.0752968777908,-0.0366714491301,-0.00508041992326,0.0590876803126,-0.116625416506,-0.0194474547958,-0.0545133996421,0.023332139766,-0.014695114347], intercept=0.0)\n",
    "\"\"\"\n",
    "#print (log_model) \n",
    "#print (nbay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "svm_predictions = featurePV.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_PErr = 100* svm_predictions.filter(lambda (v, p): v != p).count() / float(featurePV.count())\n",
    "print \"SVM Prediction Accuracy: \" + \"%.3f\" %(svm_PErr) + \"%\" \n",
    "print \"SVM  Prediction error: \" + \"%.3f\" % (100-svm_PErr) + \"%\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression Prediction Accuracy: 15.500%\n",
      "Log Regression  Prediction error: 84.500%\n"
     ]
    }
   ],
   "source": [
    "# Log\n",
    "\n",
    "log_predictions = featurePV.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_PErr = 100* log_predictions.filter(lambda (v, p): v != p).count() / float(featurePV.count())\n",
    "print \"Log Regression Prediction Accuracy: \" + \"%.3f\" %(log_PErr) + \"%\" \n",
    "print \"Log Regression  Prediction error: \" + \"%.3f\" % (100-log_PErr) + \"%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Prediction Accuracy: 84.451%\n",
      "Bayes Prediction error: 15.549%\n"
     ]
    }
   ],
   "source": [
    "# Bayes\n",
    "bayes_predictions = featurePV.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "bayes_PErr = 100.0 * bayes_predictions.filter(lambda (x, v): x == v).count() / featurePV.count()\n",
    "print \"Naive Bayes Prediction Accuracy: \" + \"%.3f\" %(bayes_PErr) + \"%\" \n",
    "print \"Bayes Prediction error: \" + \"%.3f\" % (100-bayes_PErr) + \"%\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
