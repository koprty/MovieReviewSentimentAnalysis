{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Analysis Outline\n",
    "## import necessary python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pyspark' from '/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# output.txt is created by running a python script to convert the format of the movie data\n",
    "# into a format that spark readily converts into a RDD\n",
    "\n",
    "# \"transformData.py\" is this script which is intended to be run once on our data\n",
    "import re\n",
    "import string\n",
    "from operator import add\n",
    "import os\n",
    "import sys\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import pyspark.mllib.regression as mllib_reg\n",
    "import pyspark.mllib.linalg as mllib_lalg\n",
    "import pyspark.mllib.classification as mllib_class\n",
    "import pyspark.mllib.tree as mllib_tree\n",
    "print (pyspark) # test to see that pyspark is up and running okay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_html_tags(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data.lower())\n",
    "\n",
    "#parse and take care of funky symbols in summary and text <- normalize to root words maybe?\n",
    "def cleanData (data):\n",
    "    data = strip_html_tags(data)\n",
    "    data =  re.sub(\"[\\t\\,\\:;\\(\\)\\\"\\'\\~\\-\\!\\?\\`]\", \"\",data, 0, 0)\n",
    "    data =  re.sub(\"[\\.0`]\", \"\",data, 0, 0) # special case to get rid of \".0\" of scores -- \n",
    "    return data\n",
    "\n",
    "#re.sub(\"[\\.\\t\\,\\:;\\(\\)\\.]\", \" \", strip_html_tags(data.lower()), 0, 0)\n",
    "\n",
    "# clean the data for each element of each sub list\n",
    "def prepData (list_str):\n",
    "    L= []\n",
    "    for x in list_str:\n",
    "        #print x\n",
    "        L.append(cleanData(x).strip())\n",
    "    return L\n",
    "\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "# headers for the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE THE FILE WE WANT TO USE AS OUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# CHANGED SEPARATOR TO THREE /// because / separator was causing discrepencies\n",
    "# the original file for the current output.txt is a truncated version of all our data, so the end elements will be funky\n",
    "\n",
    "# output < smaller_parsed\n",
    "#movies_txt = sc.textFile(\"output.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "movies_txt2 = sc.textFile(\"smaller_parsed.txt\").map(lambda x: (cleanData(x).split('///')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrying to parse without using a separate python file (wihtout using transformData.py)\\n\\n\\n# CHANGED SEPARATOR TO THREE /// because / separator was causing discrepencies\\n# the original file for the current output.txt is a truncated version of all our data, so the end elements will be funky\\n\\n\\n######\\n# So our first issue, the transformData.py script cannot read more than 100k data points in one go...\\n# crashs afterwards, so no where near reading the entire 7 mil we need \\n# HOW CAN WE READ THE 7 million data points and group every group of 8 datapoints together without crashing any servers\\n#### option 1 - try something with spark \\n##### -> for now we will test ~111k data points with several machine learning techniques and then get back to this problem if we have time\\n#####\\n\\n\\n\\n# movies_txt = sc.textFile(\"movies.txt\").map(lambda x: (cleanData(x)))\\n\\n# smaller_movies is the head 10000k bytes of the original movie data set\\nmovies_txt = sc.textFile(\"smaller_movies.txt\").map(lambda x: (lambda y: cleanData(\" \".join(y.strip().split(\" \")[1:]))) (x))\\n\\n\\n\\n#movies_txt = sc.textFile(\"tiniest_movies.txt\").map(lambda x: (lambda y: cleanData(\" \".join(y.strip().split(\" \")[1:]))) (x))\\n\\n# DONT TRY TO PRINT ALL THE MOVIE REVIEWS... it WILL CRASH THE KERNEL\\n# get through parsing the data and sectioning everything off before trying (or better yet, just dont try... seriously )\\n# big data = BIG and LARGE and SLOW\\n\\n\\n#print movies_test.collect()\\nmovies_txt1 = movies_txt.filter(lambda x: len(x) > 0 and x.strip() != \\'\\' ) # remove all empty strings from the rdd\\nmovies_txt2 = movies_txt1.zipWithIndex().groupBy(lambda (string, idx): idx/8 )        .map(lambda(id_num, str1):  tuple([string  for string, idx in str1  ])  )\\n\\n#movies_final_txt = movies_txt2.count()\\nprint movies_txt2.count()\\n#print movies_txt2.collect()\\n\\n\\n#### USING PARSED FILES from transformData.py\\n# output < smaller_parsed\\n#movies_txt = sc.textFile(\"output.txt\").map(lambda x: (cleanData(x).split(\\'///\\')))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "trying to parse without using a separate python file (wihtout using transformData.py)\n",
    "\n",
    "\n",
    "# CHANGED SEPARATOR TO THREE /// because / separator was causing discrepencies\n",
    "# the original file for the current output.txt is a truncated version of all our data, so the end elements will be funky\n",
    "\n",
    "\n",
    "######\n",
    "# So our first issue, the transformData.py script cannot read more than 100k data points in one go...\n",
    "# crashs afterwards, so no where near reading the entire 7 mil we need \n",
    "# HOW CAN WE READ THE 7 million data points and group every group of 8 datapoints together without crashing any servers\n",
    "#### option 1 - try something with spark \n",
    "##### -> for now we will test ~111k data points with several machine learning techniques and then get back to this problem if we have time\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "# movies_txt = sc.textFile(\"movies.txt\").map(lambda x: (cleanData(x)))\n",
    "\n",
    "# smaller_movies is the head 10000k bytes of the original movie data set\n",
    "movies_txt = sc.textFile(\"smaller_movies.txt\").map(lambda x: (lambda y: cleanData(\" \".join(y.strip().split(\" \")[1:]))) (x))\n",
    "\n",
    "\n",
    "\n",
    "#movies_txt = sc.textFile(\"tiniest_movies.txt\").map(lambda x: (lambda y: cleanData(\" \".join(y.strip().split(\" \")[1:]))) (x))\n",
    "\n",
    "# DONT TRY TO PRINT ALL THE MOVIE REVIEWS... it WILL CRASH THE KERNEL\n",
    "# get through parsing the data and sectioning everything off before trying (or better yet, just dont try... seriously )\n",
    "# big data = BIG and LARGE and SLOW\n",
    "\n",
    "\n",
    "#print movies_test.collect()\n",
    "movies_txt1 = movies_txt.filter(lambda x: len(x) > 0 and x.strip() != '' ) # remove all empty strings from the rdd\n",
    "movies_txt2 = movies_txt1.zipWithIndex().groupBy(lambda (string, idx): idx/8 )\\\n",
    "        .map(lambda(id_num, str1):  tuple([string  for string, idx in str1  ])  )\n",
    "\n",
    "#movies_final_txt = movies_txt2.count()\n",
    "print movies_txt2.count()\n",
    "#print movies_txt2.collect()\n",
    "\n",
    "\n",
    "#### USING PARSED FILES from transformData.py\n",
    "# output < smaller_parsed\n",
    "#movies_txt = sc.textFile(\"output.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parsing out relevant fields - score and text of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111101\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# prep our data\n",
    "\n",
    "#print movies_txt2.top(1)\n",
    "movies_new = movies_txt2.map(lambda L: (L[0], L[4], L[6], L[7]) if len(L) == 8 else L )\n",
    "movies_new1 = movies_new.filter(lambda L: len(L) == 4)\n",
    "# check that we aren't losing any data \n",
    "# --> we are still missing some from our parsing.... :/ \n",
    "# 9 in fact... we can fix this later\n",
    "\n",
    "print (movies_new1.count())\n",
    "print (movies_new.count() == movies_new1.count())\n",
    "\n",
    "removeHTMLTags = movies_new1.map(prepData) # remove html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVISED STOP WORDS REMOVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5', u'5']\n"
     ]
    }
   ],
   "source": [
    "#REVISED STOP WORDS REMOVAL Approach\n",
    "\n",
    "#used the stopwords file in the virtual box instead... taking too long\n",
    "baseDir = os.path.join('../data')\n",
    "inputPath = os.path.join('cs100', 'lab3')\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "#print 'These are the stopwords: %s' % stopwords\n",
    "\n",
    "def tokenize(string):\n",
    "    #reusing assignment code\n",
    "    \"\"\" An implementation of input string tokenization to exclude stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    splitList = re.split(split_regex, string.lower().strip())\n",
    "    \n",
    "    return [x for x in splitList if x!=\"\" and x not in stopwords]  \n",
    "\n",
    "\n",
    "intSW = removeHTMLTags.map(lambda x: x[1]).top(20)\n",
    "print intSW\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int((str(x[1])).replace(\"/\",\"\")), tokenize(x[3]))   )\n",
    "#print(removeSW_output.collect())\n",
    "#print (removeSW_output.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing - making the vector of features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\n",
      "(1,[0],[3.0])\n",
      "95309\n"
     ]
    }
   ],
   "source": [
    "## Took forver to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "\n",
    "print \"just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\"\n",
    "print (htf.transform([\"hi\", \"boo\", \"hi\"]));\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "print feature_vector.filter(lambda x: x.label == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXPLORE DIFFERENT ML APPROACHS\n",
    "# https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.classification-module.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Trying SVM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: REFRESHING HTF, and FEATURE VECTOR\n",
      "training error of the SVM model= 0.140778324362\n",
      "testing error of the SVM model= 0.145304511615\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, LogisticRegressionWithLBFGS \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "print (\"SVM: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "\n",
    "training_data, testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "model = SVMWithSGD.train(training_data, iterations=100) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "labelsAndPreds = training_data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(training_data.count())\n",
    "print(\"training error of the SVM model= \" + str(trainErr))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "labelsAndPreds = testing_data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(testing_data.count())\n",
    "print(\"testing error of the SVM model= \" + str(trainErr))\n",
    "\n",
    "\n",
    "\n",
    "###### TO DO \n",
    "#labelsAndPreds = feature_vector.map(lambda p: (p.label, model.predict(p.features)))\n",
    "#trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(feature_vector.count())\n",
    "#print(\"training error of the model= \" + str(trainErr))\n",
    "\n",
    "# Saving the model is meh\n",
    "#model.save(sc, \"target/tmp/pythonSVMWithSGDModel\")\n",
    "#sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionWithLBFGS: REFRESHING HTF, and FEATURE VECTOR\n",
      "Log Training Accuracy: 85.651%\n",
      "training error of the LogisticRegressionWithLBFGS model= 14.349%\n",
      "Log Testing Accuracy: 86.100%\n",
      "testing error of the LogisticRegressionWithLBFGS model= 13.900%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, LogisticRegressionWithLBFGS \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - test/update number of features\n",
    "'''\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "print (\"LogisticRegressionWithLBFGS: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "training_data, testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "model = LogisticRegressionWithLBFGS.train(training_data, iterations=100) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "labelsAndPreds = training_data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(training_data.count())\n",
    "print \"Log Training Accuracy: \" + \"%.3f\" %(100*(1-trainErr)) + \"%\" \n",
    "print(\"training error of the LogisticRegressionWithLBFGS model= \" + \"%.3f\" % (100*(trainErr))) + \"%\" \n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "labelsAndPreds = testing_data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "testErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(testing_data.count())\n",
    "\n",
    "print \"Log Testing Accuracy: \" + \"%.3f\" %(100*(1-testErr)) + \"%\" \n",
    "print(\"testing error of the LogisticRegressionWithLBFGS model= \" + \"%.3f\" %(testErr*100)) + \"%\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Do some Predictions - Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAYES: REFRESHING HTF, and FEATURE VECTOR\n",
      "Naive Bayes Accuracy: 85.809%\n",
      "Bayes error: 14.191%\n"
     ]
    }
   ],
   "source": [
    "### THIS USES NAIVE BAYES CLASSIFIER\n",
    "### with 116 elements with the first output.txt 86KB , gets 80-93%\n",
    "# tested with lots of data.... - 111MB\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "print (\"BAYES: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "# would be nice to figure out which division is the best\n",
    "training_data, testing_data= feature_vector.randomSplit([0.6, 0.4])\n",
    "#training_data, testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "# parameters:\n",
    "lamda = 1.0\n",
    "\n",
    "# build Naive Bayes Classifier\n",
    "nbay = mllib_class.NaiveBayes.train(training_data, lamda)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = testing_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "testErr = predictionAndLabel.filter(lambda (v, p): v != p).count() / float(testing_data.count())\n",
    "accuracy = 100.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / testing_data.count()\n",
    "\n",
    "print \"Naive Bayes Accuracy: \" + \"%.3f\" %(accuracy) + \"%\" \n",
    "print \"Bayes error: \" + \"%.3f\" % (100-accuracy) + \"%\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
