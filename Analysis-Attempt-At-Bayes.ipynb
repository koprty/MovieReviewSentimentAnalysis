{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Analysis Outline\n",
    "## import necessary python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output.txt is created by running a python script to convert the format of the movie data\n",
    "# into a format that spark readily converts into a RDD\n",
    "\n",
    "# \"transformData.py\" is this script which is intended to be run once on our data\n",
    "import re\n",
    "import string\n",
    "from operator import add\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pyspark' from '/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# Just notes for Lise... ignore if you arent having any spark not found problems\n",
    "# Tried Configure the necessary Spark environment\n",
    "# TOOK FOREVER TO CONFIGURE THIS.... pyspark... grrr\n",
    "# dont try unless you know vaguely what variable this takes care of \n",
    "# export SPARK_HOME=\"/usr/local/bin/spark-1.3.1-bin-hadoop2.6\"\n",
    "\"\"\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "\n",
    "\n",
    "# Add the py4j to the path.\n",
    "# You may need to change the version number to match your install\n",
    "sys.path.insert(0, os.path.join(spark_home, '/python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import pyspark.mllib.regression as mllib_reg\n",
    "import pyspark.mllib.linalg as mllib_lalg\n",
    "import pyspark.mllib.classification as mllib_class\n",
    "import pyspark.mllib.tree as mllib_tree\n",
    "\n",
    "print (pyspark) # test to see that pyspark is up and running okay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_html_tags(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data.lower())\n",
    "\n",
    "#parse and take care of funky symbols in summary and text <- normalize to root words maybe?\n",
    "def cleanData (data):\n",
    "    data = strip_html_tags(data)\n",
    "    data =  re.sub(\"[\\t\\,\\:;\\(\\)\\\"\\'\\~\\-\\!\\?\\`]\", \"\",data, 0, 0)\n",
    "    data =  re.sub(\"[\\.0`]\", \"\",data, 0, 0) # special case to get rid of \".0\" of scores -- \n",
    "    return data\n",
    "\n",
    "#re.sub(\"[\\.\\t\\,\\:;\\(\\)\\.]\", \" \", strip_html_tags(data.lower()), 0, 0)\n",
    "\n",
    "# clean the data for each element of each sub list\n",
    "def prepData (list_str):\n",
    "    L= []\n",
    "    for x in list_str:\n",
    "        #print x\n",
    "        L.append(cleanData(x))\n",
    "    return L\n",
    "\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "# headers for the project\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE THE FILE WE WANT TO USE AS OUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHANGED SEPARATOR TO THREE /// because / separator was causing discrepencies\n",
    "# the original file for the current output.txt is a truncated version of all our data, so the end elements will be funky\n",
    "\n",
    "# output < smaller_parsed\n",
    "#movies_txt = sc.textFile(\"output.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "movies_txt = sc.textFile(\"smaller_parsed.txt\").map(lambda x: (cleanData(x).split('///')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing out relevant fields - score and text of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111110\n",
      "111101\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# prep our data\n",
    "\n",
    "\n",
    "movies_new = movies_txt.map(lambda L: (L[0], L[4], L[6], L[7]) if len(L) == 8 else (\"ERROR\"))\n",
    "movies_new1 = movies_new.filter(lambda L: L!=(\"ERROR\"))\n",
    "# check that we aren't losing any data \n",
    "# --> we are still missing some from our parsing.... :/ \n",
    "# 9 in fact... we can fix this later\n",
    "print (movies_new.count())\n",
    "print (movies_new1.count())\n",
    "print (movies_new.count() == movies_new1.count())\n",
    "\n",
    "removeHTMLTags = movies_new1.map(prepData) # remove html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVISED STOP WORDS REMOVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, [u'zulu', u'superlative', u'depiction', u'19th', u'century', u'battle', u'rourkes', u'drift', u'british', u'outpost', u'heavily', u'outnumbered', u'seige', u'zulu', u'warriors', u'fought', u'desperately', u'almost', u'last', u'man', u'somehow', u'prevailed', u'terrifying', u'exciting', u'war', u'film', u'unlike', u'splendid', u'casta', u'word', u'diamond', u'entertainment', u'tapesmy', u'experience', u'slp', u'speed', u'vhs', u'videos', u'tracks', u'remarkably', u'well', u'usually', u'recorded', u'hifi', u'sound', u'well', u'standard', u'linear', u'formatzulu', u'also', u'available', u'dvdalso', u'recommendedzulu', u'dawn', u'1979', u'tells', u'complete', u'story', u'struggles', u'tribesmen', u'british', u'soldiers', u'commenced', u'1879', u'burt', u'lancaster', u'vhs', u'edition', u'dvd', u'editionparenthetical', u'number', u'preceding', u'title', u'1', u'1', u'viewer', u'poll', u'rating', u'found', u'film', u'resource', u'website8', u'zulu', u'uk1964', u'stanley', u'baker', u'jack', u'hawkins', u'ulla', u'jacobson', u'james', u'booth', u'michael', u'caine', u'nigel', u'green', u'patrick', u'magee', u'richard', u'burton', u'narrator']), (5, [u'zulu', u'superlative', u'depiction', u'19th', u'century', u'battle', u'rourkes', u'drift', u'british', u'outpost', u'heavily', u'outnumbered', u'seige', u'zulu', u'warriors', u'fought', u'desperately', u'almost', u'last', u'man', u'somehow', u'prevailed', u'terrifying', u'exciting', u'war', u'film', u'unlike', u'splendid', u'casta', u'word', u'diamond', u'entertainment', u'tapesmy', u'experience', u'slp', u'speed', u'vhs', u'videos', u'tracks', u'remarkably', u'well', u'usually', u'recorded', u'hifi', u'sound', u'well', u'standard', u'linear', u'formatzulu', u'also', u'available', u'dvdalso', u'recommendedzulu', u'dawn', u'1979', u'tells', u'complete', u'story', u'struggles', u'tribesmen', u'british', u'soldiers', u'commenced', u'1879', u'burt', u'lancaster', u'vhs', u'edition', u'dvd', u'editionparenthetical', u'number', u'preceding', u'title', u'1', u'1', u'viewer', u'poll', u'rating', u'found', u'film', u'resource', u'website8', u'zulu', u'uk1964', u'stanley', u'baker', u'jack', u'hawkins', u'ulla', u'jacobson', u'james', u'booth', u'michael', u'caine', u'nigel', u'green', u'patrick', u'magee', u'richard', u'burton', u'narrator'])]\n",
      "111101\n"
     ]
    }
   ],
   "source": [
    "#REVISED STOP WORDS REMOVAL Approach\n",
    "\n",
    "#used the stopwords file in the virtual box instead... taking too long\n",
    "baseDir = os.path.join('../data')\n",
    "inputPath = os.path.join('cs100', 'lab3')\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "#print 'These are the stopwords: %s' % stopwords\n",
    "\n",
    "def tokenize(string):\n",
    "    #recycling assignement code\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    splitList = re.split(split_regex, string.lower().strip())\n",
    "    \n",
    "    return [x for x in splitList if x!=\"\" and x not in stopwords]  \n",
    "\n",
    "\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int((str(x[1])).replace(\"/\",\"\")), tokenize(x[3]))   ).cache()\n",
    "print(removeSW_output.top(2))\n",
    "print (removeSW_output.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing - making the vector of features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,[0],[3.0])\n",
      "95309\n"
     ]
    }
   ],
   "source": [
    "## Took forver to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "\n",
    "print \"just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\"\n",
    "print (htf.transform([\"hi\", \"boo\", \"hi\"]));\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "print feature_vector.filter(lambda x: x.label == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXPLORE DIFFERENT ML APPROACHS\n",
    "# https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.classification-module.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Trying SVM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: REFRESHING HTF, and FEATURE VECTOR\n",
      "training error of the model= 0.857859065175\n",
      "training error of the model= 0.857859065175\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "print (\"SVM: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "\n",
    "training_data, testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "\n",
    "# Build the Support Vector Machine model\n",
    "model = SVMWithSGD.train(feature_vector, iterations=50) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "labelsAndPreds = feature_vector.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(feature_vector.count())\n",
    "print(\"training error of the model= \" + str(trainErr))\n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "###### TO DO \n",
    "#labelsAndPreds = feature_vector.map(lambda p: (p.label, model.predict(p.features)))\n",
    "#trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(feature_vector.count())\n",
    "#print(\"training error of the model= \" + str(trainErr))\n",
    "\n",
    "# Saving the model is meh\n",
    "#model.save(sc, \"target/tmp/pythonSVMWithSGDModel\")\n",
    "#sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Do some Predictions - Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAYES: REFRESHING HTF, and FEATURE VECTOR\n",
      "85.847718935\n"
     ]
    }
   ],
   "source": [
    "### THIS USES NAIVE BAYES CLASSIFIER\n",
    "### with 116 elements with the first output.txt 86KB , gets 80-93%\n",
    "# tested with lots of data.... - 111MB\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "print (\"BAYES: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "# would be nice to figure out which division is the best\n",
    "training_data, testing_data= feature_vector.randomSplit([0.6, 0.4])\n",
    "#training_data, testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "# parameters:\n",
    "lamda = 1.0\n",
    "\n",
    "# build Naive Bayes Classifier\n",
    "nbay = mllib_class.NaiveBayes.train(training_data, lamda)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = testing_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "testErr = predictionAndLabel.filter(lambda (v, p): v != p).count() / float(testing_data.count())\n",
    "accuracy = 100.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / testing_data.count()\n",
    "\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
