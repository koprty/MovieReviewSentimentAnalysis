{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Analysis Outline\n",
    "## import necessary python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pyspark' from '/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# \"transformData.py\" is this script which is intended to be run once on our data\n",
    "# transform the raw data into a rdd readable format first\n",
    "\n",
    "\n",
    "# import all necessary libraries\n",
    "import re\n",
    "import string\n",
    "from operator import add\n",
    "import os\n",
    "import sys\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import pyspark.mllib.regression as mllib_reg\n",
    "import pyspark.mllib.linalg as mllib_lalg\n",
    "import pyspark.mllib.classification as mllib_class\n",
    "import pyspark.mllib.tree as mllib_tree\n",
    "\n",
    "\n",
    "\n",
    "print (pyspark) # test to see that pyspark is up and running okay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of all html tags in the data\n",
    "def strip_html_tags(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data.lower())\n",
    "\n",
    "#parse and take care of funky symbols in summary and text <- normalize to root words maybe?\n",
    "def cleanData (data):\n",
    "    data = strip_html_tags(data)\n",
    "    data =  re.sub(\"[\\t\\,\\:;\\(\\)\\\"\\'\\~\\-\\!\\?\\`]\", \"\",data, 0, 0)\n",
    "    data =  re.sub(\"[\\.0`]\", \"\",data, 0, 0) # special case to get rid of \".0\" of scores -- \n",
    "    return data\n",
    "\n",
    "#re.sub(\"[\\.\\t\\,\\:;\\(\\)\\.]\", \" \", strip_html_tags(data.lower()), 0, 0)\n",
    "\n",
    "# clean the data for each element of each sub list\n",
    "def prepData (list_str):\n",
    "    L= []\n",
    "    for x in list_str:\n",
    "        #print x\n",
    "        L.append(cleanData(x).strip())\n",
    "    return L\n",
    "#####\n",
    "# Notes \n",
    "# headers for the project\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE THE FILE WE WANT TO USE AS OUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform Data uses /// as a separator for each eleent of data for each movie review\n",
    "# the original file for the current output.txt is a truncated version of all our data, so the end elements will be funky\n",
    "\n",
    "movies_txt2 = sc.textFile(\"100k_parsed.txt\").map(lambda x: (cleanData(x).split('///')))\n",
    "#movies_txt2 = sc.textFile(\"smaller_parsed.txt\").map(lambda x: (cleanData(x).split('///')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parsing out relevant fields - score and text of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99991, 100000)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# prep our data\n",
    "\n",
    "#####\n",
    "# Notes \n",
    "# headers for the project\n",
    "# [u'productId', u'userId', u'profileName', u'helpfulness', u'score', u'time', u'summary', u'text']\n",
    "# [u'productId', _ , _ , _ , u'score', _  u'summary', u'text']\n",
    "\n",
    "# take the data that we care most about\n",
    "movies_new = movies_txt2.map(lambda L: (L[0], L[4], L[6], L[7]) if len(L) == 8 else L )\n",
    "movies_new1 = movies_new.filter(lambda L: len(L) == 4) # lets just take all the data that has been parsed correctly\n",
    "\n",
    "#print (movies_new.top(20))\n",
    "print (movies_new1.count(), movies_new.count())\n",
    "print (movies_new.count() == movies_new1.count())\n",
    "\n",
    "removeHTMLTags = movies_new1.map(prepData) # remove html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVISED STOP WORDS REMOVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#REVISED STOP WORDS REMOVAL Approach\n",
    "\n",
    "#used the stopwords file in the virtual box instead... taking too long\n",
    "baseDir = os.path.join('../data')\n",
    "inputPath = os.path.join('cs100', 'lab3')\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    #reusing assignment code\n",
    "    \"\"\" An implementation of input string tokenization to exclude stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    splitList = re.split(split_regex, string.lower().strip())\n",
    "    \n",
    "    return [x for x in splitList if x!=\"\" and x not in stopwords]  \n",
    "\n",
    "\n",
    "\n",
    "removeSW_output= removeHTMLTags.map(lambda x: (  int((str(x[1])).replace(\"/\",\"\")), tokenize(x[3]))   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing - making the vector of features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\n",
      "(1,[0],[3.0])\n",
      "85876\n"
     ]
    }
   ],
   "source": [
    "# We will use the spark ml library for this (took a bit long to figure out)\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "\n",
    "# HashingTF calulates the tfid feature for us\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "\n",
    "print \"just testing the hashing TF transformation on the array: ['hi', 'boo', 'hi']\"\n",
    "print (htf.transform([\"hi\", \"boo\", \"hi\"]));\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "\n",
    "#feature_vector = removeSW3.map(lambda elements: LabeledPoint(1, [2]))\n",
    "print feature_vector.filter(lambda x: x.label == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXPLORE DIFFERENT ML APPROACHS\n",
    "# https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.classification-module.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Trying SVM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, LogisticRegressionWithLBFGS \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "print (\"SVM: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "\n",
    "svm_training_data, svm_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "svm_model = SVMWithSGD.train(svm_training_data, iterations=500) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "svm_labelsAndPreds = svm_training_data.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_trainErr = 100* svm_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(svm_training_data.count())\n",
    "print(\"training error of the SVM model: \" + \"%.3f\" %(svm_trainErr)) + \"%\" \n",
    "print(\"training accuracy of the SVM model: \" + \"%.3f\" %(100-svm_trainErr)) + \"%\" \n",
    "\n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "svm_labelsAndPredsTest = svm_testing_data.map(lambda p: (p.label, svm_model.predict(p.features)))\n",
    "svm_testErr = 100* svm_labelsAndPredsTest.filter(lambda (v, p): v != p).count() / float(svm_testing_data.count())\n",
    "print(\"testing error of the SVM model: \" + \"%.3f\" %(svm_testErr)) + \"%\" \n",
    "print(\"training accuracy of the SVM model: \" + \"%.3f\" %(100-svm_testErr)) + \"%\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, LogisticRegressionWithLBFGS \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - test/update number of features\n",
    "'''\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "print (\"LogisticRegressionWithLBFGS: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "log_training_data, log_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "#LogisticRegressionWithLBFGS \n",
    "# Build the Support Vector Machine model\n",
    "log_model = LogisticRegressionWithLBFGS.train(log_training_data, iterations=500) # lower iterations so we arent here forever\n",
    "\n",
    "# Evaluate the model on training data\n",
    "log_labelsAndPreds = log_training_data.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_trainErr = 100* log_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(log_training_data.count())\n",
    "print \"Log Training Accuracy: \" + \"%.3f\" %(100-log_trainErr) + \"%\" \n",
    "print(\"training error of the LogisticRegressionWithLBFGS model= \" + \"%.3f\" % (log_trainErr)) + \"%\" \n",
    "\n",
    "#Evaluate on testing data.. the correct way lolz\n",
    "\n",
    "log_labelsAndPredsTest = log_testing_data.map(lambda p: (p.label, log_model.predict(p.features)))\n",
    "log_testErr = 100* log_labelsAndPredsTest.filter(lambda (v, p): v != p).count() / float(log_testing_data.count())\n",
    "\n",
    "print \"Log Testing Accuracy: \" + \"%.3f\" %(100-log_testErr) + \"%\" \n",
    "print(\"testing error of the LogisticRegressionWithLBFGS model: \" + \"%.3f\" %(log_testErr)) + \"%\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Do some Predictions - Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### THIS USES NAIVE BAYES CLASSIFIER\n",
    "### with 116 elements with the first output.txt 86KB , gets 80-93%\n",
    "# tested with lots of data.... - 111MB\n",
    "\n",
    "'''  REFRESH BLOCK\n",
    "REFRESHING DATA\n",
    "Re Run data -> refresh in case we are using another approach above this as well\n",
    "Things to do - update number of features\n",
    "'''\n",
    "## Took forever to figure out\n",
    "## key notes, use mllib if you are using rdd, ditch ml if you are not using data frams\n",
    "htf = HashingTF(numFeatures=1) # features need to match dimensions of output\n",
    "print (\"BAYES: REFRESHING HTF, and FEATURE VECTOR\")\n",
    "feature_vector = removeSW_output.map(lambda elements: LabeledPoint(1.0, [htf.transform(elements[1])]) if float(elements[0]) >= 2.5 else LabeledPoint(0.0, [htf.transform(elements[1])]))\n",
    "'''\n",
    "REFRESHED DATA\n",
    "'''\n",
    "\n",
    "# would be nice to figure out which division is the best\n",
    "#training_data, testing_data= feature_vector.randomSplit([0.6, 0.4])\n",
    "bayes_training_data, bayes_testing_data= feature_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "# parameters:\n",
    "lamda = 1.0\n",
    "\n",
    "# build Naive Bayes Classifier\n",
    "nbay = mllib_class.NaiveBayes.train(bayes_training_data, lamda)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bayes_labelsAndPreds = bayes_training_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "#bayes_trainErr = bayes_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(bayes_training_data.count())\n",
    "bayes_trainErr = 100.0 * bayes_labelsAndPreds.filter(lambda (x, v): x == v).count() / bayes_training_data.count()\n",
    "\n",
    "print \"Naive Bayes Training Accuracy: \" + \"%.3f\" %(bayes_trainErr) + \"%\" \n",
    "print \"Bayes Training error: \" + \"%.3f\" % (100-bayes_trainErr) + \"%\" \n",
    "\n",
    "\n",
    "# Testing Data Accuracy\n",
    "\n",
    "bayes_labelsAndPreds = bayes_testing_data.map(lambda p : (nbay.predict(p.features), p.label))\n",
    "#bayes_testErr = bayes_labelsAndPreds.filter(lambda (v, p): v != p).count() / float(bayes_testing_data.count())\n",
    "bayes_testErr = 100.0 * bayes_labelsAndPreds.filter(lambda (x, v): x == v).count() / bayes_testing_data.count()\n",
    "\n",
    "\n",
    "print \"Naive Bayes Testing Accuracy: \" + \"%.3f\" %(bayes_testErr) + \"%\" \n",
    "print \"Bayes Testing error: \" + \"%.3f\" % (100-bayes_testErr) + \"%\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
